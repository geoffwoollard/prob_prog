{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prob_prog_hw1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPU1K8bDQzsnUZqLKVuV2zP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geoffwoollard/prob_prog/blob/main/hw/hw1/prob_prog_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHvLzFS64kvr"
      },
      "source": [
        "# Probabilistic Programming\n",
        "\n",
        "# Homework 1\n",
        "* https://www.cs.ubc.ca/~fwood/CS532W-539W/homework/1.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoHdLW7h4xTj"
      },
      "source": [
        "# Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELvOXmFw43lO"
      },
      "source": [
        "**Show that the Gamma distribution is conjugate to the Poisson distribution.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjKy4u_i49j1"
      },
      "source": [
        "The Gamma distribution is over parameter $x \\in (0, \\infty)$ and the Poisson distribution is over $k \\in [0,1,2,...]$ with parameter $\\lambda \\in (0, \\infty)$. Thus it makes sense to treat Gamma as a prior $p(\\lambda)$ over $\\lambda=x$ and Poisson as the likelihood $p(k|\\lambda)$. In this case the questions is asking for us to show that the posterior $p(k|\\lambda)$.\n",
        "\n",
        "$$\n",
        "\\lambda \\sim Gamma[\\alpha,\\beta] \\\\\n",
        "p(\\lambda ) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}\\exp[-\\beta\\lambda]\n",
        "$$\n",
        "\n",
        "$$\n",
        "k|\\lambda \\sim Poisson[\\lambda] \\\\\n",
        "p(k|\\lambda) = \\frac{\\lambda^k\\exp^{-\\lambda}}{k!}\n",
        "$$\n",
        "\n",
        "We can already see a hint of the conjugacy through the $\\lambda$ to some power, and $\\lambda$ in $\\exp$.\n",
        "\n",
        "Let's apply Bayes rule.\n",
        "\n",
        "$$\n",
        "p(\\lambda|k) = \\frac{p(k|\\lambda)p(\\lambda )}{p(k)}\n",
        "$$\n",
        "\n",
        "Conjugacy is shown by the posterior being Gamma distributed: $\\lambda \\sim Gamma[\\alpha^\\prime,\\beta^\\prime]$. \n",
        "\n",
        "Gamma and Poisson have normalization constants, and the evidence $p(k)$ is unknown (but doesn't depend on $\\lambda$ because it's been marginzlized out). However, because the posterior is a distribution over $\\lambda$, we can drop all multilicative constants over $k,\\alpha,\\beta$. If we can show that the posterior has the unnormalized form of the Gamma distribution with some new parameters $\\alpha^\\prime$ and $\\beta^\\prime$, which are constant in \\lambda, but can contain $k,\\alpha,\\beta$, then we can use the known form for the normalization constant.\n",
        "\n",
        "$$\n",
        "p(\\lambda|k) \\propto \n",
        "\\big[ \\lambda^{\\alpha-1}\\exp[-\\beta\\lambda] \\big]\n",
        "\\big[ \\lambda^k\\exp^{-\\lambda} \\big]\n",
        "= \\lambda^{(\\alpha+k)-1}\\exp[-(\\beta+1)\\lambda]\n",
        "$$\n",
        "\n",
        "Thus we see that the Poisson likelihood of $k|\\lambda)$ has Gamma as a conjugate prior for $\\lambda|\\alpha,\\beta$, because the Posterior is Gamma distributed with the new parameters $\\alpha^\\prime = k + \\alpha$ and $\\beta^\\prime = \\beta + 1$.\n",
        "\n",
        "$$\n",
        "\\lambda \\sim Gamma[\\alpha^\\prime,\\beta^\\prime] \\\\\n",
        "p(\\lambda ) = \\frac{{\\beta^\\prime}^\\alpha}{\\Gamma(\\alpha^\\prime)}\\lambda^{\\alpha^\\prime-1}\\exp[-\\beta^\\prime\\lambda]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI1A2EoW0n85"
      },
      "source": [
        "# Question 2\n",
        "**Show that the Gibbs transition operator satisfies the detailed balance equation and as such can be interpreted as an MH transition operator that always accepts.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b51Q5dQ_0wdg"
      },
      "source": [
        "Let's first clarify what the questions is asking for. In Gibbs there is \n",
        "  * the individual update of a component, based on the conditional for that component with the others frozen at what they are\n",
        "  * the update of all the components after all the components have been updated.\n",
        "\n",
        "So this questions can be taken in a two senses, and I will answer it as follows.\n",
        "\n",
        "I will first show that each Gibbs updtae of the one component \"satisfies details balance and as such can be interpreted as an MH transition operator that always accepts\". Then I will consider what statements we can make on the update of the whole vector (all the components). There's an important subelty there, but the nuances can get quite academic, and I leave off with some important links that further clarify the issue (which I did read and can proudly say I understand).\n",
        "\n",
        "First, let's consider updating one component of a vector, as we do in Gibbs, and define some notation (following Detailed balance for Gibbs sampling in http://www.gatsby.ucl.ac.uk/teaching/courses/ml1-2015/lect12-handout.pdf). \n",
        "  * $T[x,x^*]$ is the transition from vector $x = (x_1,...,x_i,...x_n)$ to $x^* = (x_1,...,x_i^*,...x_n)$, where the ith component of x has been chosen.\n",
        "  * $\\pi_i$ refers to the probability of choosing to update component i. If we randomly chose a components, this would be constant. I'm not sure how to write this down for if we cycled throgh in some fixed order.\n",
        "  * $p(x_i^*|x_{-i})$ is the conditional probability of updating the component $x_i$ to $x_i^*$ given $x_{-i})$, which is the states of all the components of $x$, but without the ith component.\n",
        "\n",
        "$$\n",
        "T[x,x^*] = \\pi_ip(x_i^*|x_{-i})\n",
        "$$\n",
        "\n",
        "Detailed balance for a step in Gibbs is defined as, where $p(x)$ is the probability of being in state $x$.\n",
        "$$\n",
        "T[x^*,x]p(x^*) = T[x,x^*]p(x)\n",
        "$$\n",
        "\n",
        "The key step to show detailed balance (in one Gibbs step) is to note that only the ith component changes, and so $x_{-i} = x^*_{-i}$. Then we can just condition write $p(x)$ and $p(x^*)$ to condition on this, respectively, and note they are equal.\n",
        "$$\n",
        "T[x,x^*]p(x) \\\\\n",
        "= \n",
        "\\big[\\pi_ip(x_i^*|x_{-i})\\big]\n",
        "\\big[p(x_i|x_{-i})p(x_{-i}) \\big]\n",
        "\\\\\n",
        "= \n",
        "\\big[\\pi_ip(x_i|x{^*}_{-i})\\big]\n",
        "\\big[p(x_i|x^*_{-i})p(x^*_{-i}) \\big] \\ \\ \\text{swapping} x^*_{-i} \\rightarrow x_{-i} \n",
        "\\\\\n",
        "= \\big[\\pi_ip(x_i|x^*_{-i})\\big]\n",
        "\\big[p(x_i|x{^*}_{-i})p(x^*_{-i}) \\big]\n",
        "\\ \\ \\text{rearanging conditional terms}\n",
        "\\\\\n",
        "= T[x^*,x]p(x^*)\n",
        "$$\n",
        "\n",
        "Note that nothing has been assumed about $\\pi_i$ here, and thus no matter how we choose to update the ith component (randomly, cycle through), details balane will hold for one Gibbs step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azLuhjF0lrtZ"
      },
      "source": [
        "Showing that details balance hold for a whole Gibbs sweep (where we update all the components) is a different matter. Actually it can be shown for the case when the choice is random, and the Gibbs sampler for one update from $x_t=(x^t_1,...,x^t_n)$ to $x_{t+1} = (x_1^{t+1},...,x_n^{t+1})$ is the average of the Gibbs updates, instead of their product sum. In the latter case, there can be simple counter examples provided that show detail balance does not hold.\n",
        "\n",
        "See \n",
        "  * section 12.4 in https://statweb.stanford.edu/~owen/mc/Ch-MCMC.pdf\n",
        "  * slide 3 of http://galton.uchicago.edu/~eichler/stat24600/Handouts/s09.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiiFtliglGWf"
      },
      "source": [
        "\n",
        "Now that we've established detailed balance, the next aspect of the questions is to show that the MH acceptance operator is always one\n",
        "$$\n",
        "A = \\min\\bigg(1,\\frac{p(x^*)g(x|x^*)}{p(x)g(x^*|x)}\\bigg)\n",
        "$$\n",
        "\n",
        "$p$ is the stationary distribution. We can show that this transition operator satisfies detailed balance:\n",
        "$$\n",
        "p(x^*)T[x^*,x] = p(x)T[x,x^*]  \\ \\ \\text{(detailed balance)}\n",
        "\\\\\n",
        "p(x^*)p(x_t|x^*) = p(x_t)p(x^*|x_t)\\ \\ \\text{(T is cond. prob in Gibbs)}\n",
        "\\\\\n",
        "p(x^*,x_t) = p(x_t,x^*)\n",
        "$$\n",
        "\n",
        "$q$ is the proposal distribution. In Gibbs, the proposal distribution *is* the stationary distribution, i.e. $p(a|b)=\\pi_iq(a|b)$, because the transition operator is the conditional of the stationary distribution (and the probability of choosing to update the ith component).\n",
        "\n",
        "From detailed balance we have \n",
        "$$\n",
        "\\pi_ip(x|x^*) p(x^*) = \\pi_ip(x)p(x^*|x)\n",
        "\\\\\n",
        "\\implies \\frac{p(x^*)g(x|x^*)}{p(x)g(x^*|x)} = 1\n",
        "$$\n",
        "\n",
        "And since $p=q$ in Gibbs we have \n",
        "$$\n",
        "A = \\min(1,1) = 1\n",
        "$$\n",
        "\n",
        "Which means that the MH transition operator in this case always accepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as-Gzp-g4rc6"
      },
      "source": [
        "# Question 3\n",
        "**Write code to compute the probability three ways that it is cloudy given that we observe that the grass is wet using this Bayes net model.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wG00S-_4g04"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2eIQtyh4_NK"
      },
      "source": [
        "##first define the probability distributions as defined in the excercise:\n",
        "\n",
        "#define 0 as false, 1 as true\n",
        "def p_C(c):\n",
        "    p = np.array([0.5,0.5])\n",
        "    return p[c]\n",
        "\n",
        "def p_S_given_C(s,c):\n",
        "    p = np.array([[0.5,0.9],[0.5,0.1]])\n",
        "    return p[s,c]\n",
        "    \n",
        "def p_R_given_C(r,c):\n",
        "    p = np.array([[0.8,0.2],[0.2,0.8]])\n",
        "    return p[r,c]\n",
        "\n",
        "def p_W_given_S_R(w,s,r):\n",
        "    p = np.array([\n",
        "            [[1.0,0.1],[0.1,0.01]],   #w = False\n",
        "            [[0.0,0.9],[0.9,0.99]],   #w = True\n",
        "            ])\n",
        "    return p[w,s,r]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6Ssri_n47zp"
      },
      "source": [
        "**By enumerating all possible world states and conditioning by counting which proportion are cloudy given the observed world characteristic, namely, that the grass is wet.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9BH_vrw9JgW"
      },
      "source": [
        "The joint is the enumeration of all possible world states, and the proportion. It is possible to enumerate all these world states because the we can loop through the small number of discrete states (binary in this example) in this small graphical model (just four nodes in this case).\n",
        "\n",
        "The complexity of enumerating a grapihcal model with $n_{nodes}$ with node i having $n_{states}^i$ number of discrete states is $o(\\prod_i n_{states}^i)$. This is how many multiplications there are. Note that we can precompute these states and don't have to keep calling the functions to compute them.\n",
        "\n",
        "We just need to \n",
        "* condition by fixing cloudy as True\n",
        "* count by taking the ratio of probabilities of the marginals $\\frac{p(w=1,c=1)}{p(w=1)} = \\frac{\\sum_{s,r}p(c=1,s,r,w=1)}{\\sum_{c,s,r}p(c,s,r,w=1)}$ from Bayes rule.\n",
        "\n",
        "Once we have the joint, it is easy to get the marginal by just summing over specific axes.\n",
        "\n",
        "This gives us an \"exact\" probability, since it uses the joint, which comes from the conditional probability tables. Other sampling proceduces (MCMC/Gibbs) that use the conditionals will reach in the limit of many random iterations. This is a siple example, and many times the graph has too many nodes, or the nodes have too many states, the the joint is not directly available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1igXngg5Eh1",
        "outputId": "467a3b2f-521e-4164-8613-98f02cabac8b"
      },
      "source": [
        "##1. enumeration and conditioning:\n",
        "    \n",
        "## compute joint:\n",
        "p = np.zeros((2,2,2,2)) #c,s,r,w\n",
        "for c in range(2):\n",
        "    p_C_precompouted = p_C(c)\n",
        "    for s in range(2):\n",
        "        p_S_given_C_precompouted = p_S_given_C(s,c)\n",
        "        for r in range(2):\n",
        "            p_R_given_C_precompouted = p_R_given_C(r,c)\n",
        "            for w in range(2):\n",
        "                p[c,s,r,w] = p_C_precompouted*p_S_given_C_precompouted*p_R_given_C_precompouted*p_W_given_S_R(w,s,r)\n",
        "                \n",
        "## condition and marginalize:\n",
        "\n",
        "w=1 # given the grass is wet\n",
        "c=1 # query 'is it cloudy?'\n",
        "p_C_given_W = p[c,:,:,w].sum() / p[:,:,:,w].sum() # p(c=1,w=1)/p(w=1)\n",
        "\n",
        "print('There is a {:.2f}% chance it is cloudy given the grass is wet'.format(p_C_given_W*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There is a 57.58% chance it is cloudy given the grass is wet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWwldYKO-LTU"
      },
      "source": [
        "**Using ancestral sampling and rejection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bm4faix-VTs"
      },
      "source": [
        "We can do ancestral sampling with rejection as follows\n",
        "1. Start at the root of the graph (node with no parents). \n",
        "2. Count the number of cloudy days (prob is proportion of cloudy vs all), while also \n",
        "3. rejecting if grass not wet (if the graph continued on we would stop).\n",
        "\n",
        "We sample both the the R and S nodes inside the loop with C, because they depend on C, but in parallel with each other.\n",
        "\n",
        "We sample c by setting it to Uni[0,1] > \n",
        "equivalent to `np.random.choice([0,1],p=[p_C[0],p_C[1]])`\n",
        "\n",
        "Here we don't fix wet, so there is a loss of efficiency in so far as the probability of being wet is rare. Quantitatively we reject ~35% of the samples. If it's very rare, then we reject a lot and we have to sample a lot.\n",
        "\n",
        "We get the probability of cloudy given wet by accumulating the number of wet days and the number of cloudy given wet days. The probability of cloudy given wet, is the number of days that are cloudy given wet (`num_cloudy_given_grass_wet`) divided by the number of wet days (`num_wet`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i40CBcy3-Lw9",
        "outputId": "c5841774-f70d-4eef-9be5-386dfe5fe95e"
      },
      "source": [
        "num_rv_to_sample = 4 # C,S,R\n",
        "num_total_days = 1000000\n",
        "c_idx, s_idx, r_idx, w_idx = range(num_rv_to_sample)\n",
        "uni_rv = np.random.uniform(low=0,high=1,size=num_total_days*num_rv_to_sample).reshape(num_total_days,num_rv_to_sample)\n",
        "\n",
        "num_cloudy_given_grass_wet=0\n",
        "rejections=0\n",
        "num_wet = 0\n",
        "for day in range(num_total_days):\n",
        "  c = int(uni_rv[day,c_idx] > p_C(0))\n",
        "  s = int(uni_rv[day,s_idx] > p_S_given_C(0,c))\n",
        "  r = int(uni_rv[day,r_idx] > p_R_given_C(0,c))\n",
        "  w = int(uni_rv[day,w_idx] > p_W_given_S_R(0,s,r))\n",
        "  if w:\n",
        "    num_wet += 1\n",
        "    if c: \n",
        "      num_cloudy_given_grass_wet += 1\n",
        "  else:\n",
        "    rejections += 1\n",
        "#print('It is cloudy %.1f%% of the time, given it is wet' % ()\n",
        "\n",
        "print('The chance of it being cloudy given the grass is wet is {:.2f}%'.format(100*num_cloudy_given_grass_wet / num_wet))\n",
        "print('{:.2f}% of the total samples were rejected'.format(100*rejections/num_total_days))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The chance of it being cloudy given the grass is wet is 57.64%\n",
            "35.24% of the total samples were rejected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTkVrtgPS5eK"
      },
      "source": [
        "**Using Gibbs sampling**\n",
        "\n",
        "Since we have the full joint, we can easily get any conditional we want by dividing the joint by the joint marginalized by variable we are conditioning on.\n",
        "\n",
        "Although the starter code mentions some things about the zero in the conditional probability of p(w|s=0,w=0), this is not an issue in the conditional $p(c|s,r)$ since we marginalize out w:\n",
        "$$\n",
        "p(c|s,r) = \\frac{p(c,s,r)}{p(s,r)} = \\frac{\\sum_{w} p(c,s,r,w)}{\\sum_{c,w}p(c,s,r,w)}\n",
        "$$\n",
        "\n",
        "Doing Gibbs with w=1 means that we always leave it fixed, and thus only spend time doing useful sampling that we won't throw away."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjDpmlBuS2dM"
      },
      "source": [
        "#3: Gibbs\n",
        "# we can use the joint above to condition on the variables, to create the needed\n",
        "# conditional distributions:\n",
        "\n",
        "\n",
        "#we can calculate p(R|C,S,W) and p(S|C,R,W) from the joint, dividing by the right marginal distribution\n",
        "#indexing is [c,s,r,w]\n",
        "p_R_given_C_S_W = p/p.sum(axis=2, keepdims=True) # sum over r to get p(c,s,w)\n",
        "p_S_given_C_R_W = p/p.sum(axis=1, keepdims=True) # note that the order of indexing remains the same p_S_given_C_R_W[c,s,r,w]\n",
        "\n",
        "\n",
        "# but for C given R,S,W, there is a 0 in the joint (0/0), arising from p(W|S,R)\n",
        "# but since p(W|S,R) does not depend on C, we can factor it out:\n",
        "#p(C | R, S) = p(R,S,C)/(int_C (p(R,S,C)))\n",
        "\n",
        "#first create p(R,S,C):\n",
        "p_C_S_R = np.zeros((2,2,2)) #c,s,r\n",
        "for c in range(2):\n",
        "    for s in range(2):\n",
        "        for r in range(2):\n",
        "            p_C_S_R[c,s,r] = p_C(c)*p_S_given_C(s,c)*p_R_given_C(r,c)\n",
        "        \n",
        "#then create the conditional distribution:\n",
        "p_C_given_S_R_starter_code = p_C_S_R[:,:,:]/p_C_S_R[:,:,:].sum(axis=(0),keepdims=True)\n",
        "p_C_given_S_R = p.sum(axis=(-1), keepdims=True) / p.sum(axis=(0,-1), keepdims=True)\n",
        "\n",
        "assert np.allclose(p_C_given_S_R_starter_code,p_C_given_S_R[:,:,:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_Ve3JYP51LE",
        "outputId": "7d84ff0b-ae2a-419f-d851-86e83e6a30bd"
      },
      "source": [
        "p_C_given_S_R.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 2, 2, 1)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyhtAOVVBBDz"
      },
      "source": [
        "We run Gibbs for 100 000 samples, and use a burn in of 500, and only take every 10th sample.\n",
        "\n",
        "At this point I don't know how reasonable these values are, except to compare the final answer with the 57.6% obtained previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twLKF7K8WWdX",
        "outputId": "d4c32af1-4fbd-4d00-82c4-54bda19844e2"
      },
      "source": [
        "\n",
        "##gibbs sampling\n",
        "num_samples = 1000000\n",
        "samples = np.zeros(num_samples)\n",
        "state = np.zeros(4,dtype='int')\n",
        "\n",
        "num_rv_to_sample=4\n",
        "uni_rv = np.random.uniform(low=0,high=1,size=num_samples*num_rv_to_sample).reshape(num_samples,num_rv_to_sample)\n",
        "c_idx, s_idx, r_idx, w_idx = range(num_rv_to_sample)\n",
        "\n",
        "c,s,r = np.random.choice([0,1],p=[0.5,0.5],size=3)\n",
        "w=1\n",
        "\n",
        "for sample in range(num_samples):\n",
        "  c = int(uni_rv[sample,c_idx] > p_C_given_S_R[0,s,r])\n",
        "  s = int(uni_rv[sample,s_idx] > p_S_given_C_R_W[c,0,r,w])\n",
        "  r = int(uni_rv[sample,r_idx] > p_R_given_C_S_W[c,s,0,w])\n",
        "  samples[sample] = c\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The chance of it being cloudy given the grass is wet is 58.26%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FM0o864Bzw4",
        "outputId": "f116ee19-4049-48d3-b512-43c38639773c"
      },
      "source": [
        "burn_in = 10000\n",
        "stride = 10\n",
        "print('The chance of it being cloudy given the grass is wet is {:.2f}%'.format(samples[burn_in::stride].mean()*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The chance of it being cloudy given the grass is wet is 57.74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvCWo5FIMkJl"
      },
      "source": [
        "# Question 4\n",
        "**Consider the Bayesian linear regression model discussed in the lecture on graphical models**\n",
        "\n",
        "I will answer the question \"backwards\" since the results of part c are used in b and b in a."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuTaSd6KMrKD"
      },
      "source": [
        "## c) **produce the analytic form of the posterior predictive.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwh8_ZM8MxHM"
      },
      "source": [
        "The posterior predictive is the probability of $\\hat t$ given all the data we know $(\\{x_n,t_n\\},\\sigma^2,\\alpha,\\hat  x)$. Notice that $w$ is not in there. That is becuase it is unobserved and we only have a distribution for what it is. Compared with typical least squares regression, in this question we don't use a point estimate for it, but the whole probability distribution. This takes some care, because we have a multivariate Gaussian prior on $w$, and we have an updated posterior on $w$ based on our observed data. Integrating over this will involve a multidimensional integral. Let's see how this works.\n",
        "\n",
        "In the notation below, we drop $\\alpha$ and $\\sigma$, because they are always given\n",
        "$$\n",
        "p(\\hat t | \\{x_n,t_n\\},\\hat  x) = \\int dw \\ p(\\hat t | w, \\hat x) p(w|\\{x_n,t_n\\})\n",
        "$$\n",
        "\n",
        "The posterior of w is proportional to the posterior of all the $\\{t_n\\}$ and the prior of $w$ by Bayes rule. The evidence $p(t_n)$ is a constant in $w$ and doesn't affect how we compute the integral over $w$.\n",
        "\n",
        "$$\n",
        "p(w|\\{x_n,t_n\\}) \\propto  \\Bigg[\\prod_n N[t_n|w^Tx_n,\\sigma^2]\\Bigg] N[w|0,\\alpha]\n",
        "$$\n",
        "\n",
        "We can furthermore show that the posterior of w is a multivariate Gaussian with mean $w_N = \\frac{1}{\\sigma^2}V_NX^Tt$ and covariance matrix $V_N = (\\frac{X^TX}{\\sigma^2} + \\frac{I}{\\alpha})^{-1}$, where $t \\in \\mathbb{R}^N$ is the vector of all $t_n \\in \\mathbb{R}$ and $X \\in \\mathbb{R}^{n \\ x \\ d}$ is the data matrix of all $x_n \\in \\mathbb{R}^d$. To show that this is the case, it suffices to show that argument in the exponent of the posterior of $w$ can be written of the form \n",
        "$$\n",
        "-\\frac{1}{2}(w - w_N)^T V_N^{-1} (w - w_N)\\\\\n",
        "= -\\frac{1}{2}w^TV_N^{-1}w + w_N V_N^{-1}w -\\frac{1}{2}w_N^TV_N^{-1}w_N\n",
        "$$\n",
        "\n",
        "Neglecting terms that won't affect the integral, and noting that $\\sum_n x_nx_n^T = X^TX$ and $\\sum_n t_nx_n = X^Tt$, we have \n",
        "$$\n",
        "\\log p(w|\\{x_n,t_n\\}) = -\\frac{1}{2\\sigma^2} \\Sigma_n(t_n-w^Tx_n)^2  -\\frac{1}{2}w^T\\frac{I}{\\alpha}w + ... \\\\\n",
        "= -\\frac{1}{2}w^T(\\frac{X^TX}{\\sigma^2} + \\frac{I}{\\alpha})w + \\frac{t^TXw}{\\sigma^2} + ...\n",
        "$$\n",
        "\n",
        "Completing the square by matching the coefficients in the multidimensional case, we have $V_N^{-1} = \\frac{X^TX}{\\sigma^2} + \\frac{I}{\\alpha}$ as desired, and $w_N^TV_N^{-1} = \\frac{t^TX}{\\sigma^2} \\implies w_N = \\frac{1}{\\sigma^2}V_NX^Tt$ as desired (using the symmetry of $V_N$ and its inverse).\n",
        "\n",
        "Thus $p(w|\\{x_n,t_n\\}) = N[w|w_N,V_N]$\n",
        "\n",
        "To get the posterior in $\\hat t$, we have to integrate over w. \n",
        "$$\n",
        "\\int \\ dw N[\\hat t|w^Tx,\\sigma^2]N[w|w_N,V_N]\n",
        "$$\n",
        "\n",
        "This is of the form of Eq 7.60 in Murphy, 2012, and the integral is another Gaussian in $\\hat t$. Doing this integral by completing the square involves resolving the inverse of a matrix. Exact details of this are given in Morphy, 2012 (4.3.4.1 Inverse of a partitioned matrix using Schur complements; 4.3.4.2 The matrix inversion lemma) By Eq. 7.61 in Murphy, 2012, we have\n",
        "\n",
        "$$\n",
        "p(\\hat t|w,\\hat x, \\sigma,\\{t_n,x_n\\}) = N[\\hat t|w_N \\hat x,\\sigma_N^2(\\hat x) = \\sigma^2+\\hat x^TV_N \\hat x] \n",
        "\\\\\n",
        "= \\frac{1}{\\sqrt{(2\\pi\\sigma_N^2)}}\\exp[\\frac{-(\\hat t - w_N^T\\hat x)^2}{2\\sigma_N^2}]\n",
        "$$\n",
        "\n",
        "Note that $\\hat t$ is centred around the Ridge regression least squares estimate for the weights, but its uncertainty is dependent on its own data point $\\hat x$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtqAdxNQssb5"
      },
      "source": [
        "## b) **perform pure Gibbs on both w and t̂**\n",
        "\n",
        "NB: instead of using $w_t$, $w_{t+1}$ (and likewise for $\\hat t$), I just use $w$ and $w^*$ with the understanding that $\\hat x, x, t, \\sigma^2,\\alpha$ do not change for MCMC steps.\n",
        "\n",
        "To answer this question, we need to derive conditional probabilites for $w$ and $\\hat t$. This will allow us to update them in Gibbs. For the remaining variables in the graphical model, we have observed them and can use these values.\n",
        "\n",
        "$$\n",
        "p(\\hat t^* | w,\\hat x, x, t, \\sigma^2,\\alpha)\n",
        "\\\\\n",
        "p(w | \\hat t^*,\\hat x, x, t, \\sigma^2,\\alpha) \\ \n",
        "$$\n",
        "\n",
        "note the use of the new update for $\\hat t$, denoted  $\\hat t^*$ in the conditional for $w$\n",
        "\n",
        "For the conditional of $\\hat t$ we can drop $x,t,\\alpha$ since $w$ and $\\sigma^2$ are given. Then we just have the equation given for $p(t_n|w,x_n,\\sigma^2)$ with $t_n \\rightarrow \\hat t$, and $x_n \\rightarrow \\hat x$, and $w$ from the latest MCMC step.\n",
        "\n",
        "$p(\\hat t | w,\\hat x, x, t, \\sigma^2,\\alpha) = N[w^T\\hat x,\\sigma^2]$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTDEuEopwxMw"
      },
      "source": [
        "For the conditional of w, $\\hat t$ does supply some information about it, and it will change depending on that MCMC step we are at. As we cycle through Gibbs, we will use our estimate of it at the appropriate time step, and so we want to know how to use it in the conditinoal of $w$.\n",
        "\n",
        "We can use the result derived in part c), with the caveat that the $\\{t_n,x_n\\}_{n=1}^{N+1}$ data now contains the $\\{\\hat t^*, \\hat x\\}$ pair, indexed $N+1$, making sure to use the most recent samle of $\\hat t^*$ from the last MCMC step.\n",
        "\n",
        "Thus \n",
        "  * $p(w|\\{x_n,t_n\\},\\hat t^*,\\hat x,\\sigma^2,\\alpha) = N[w|w_{N+1},V_{N+1}]$\n",
        "  * with  $w_{N+1}$ and $V_{N+1}$ as defined in c), but with $X$ and $t$ including $\\{\\hat t^*, \\hat x \\}$, and thus changing every MCMC iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuGkRNgG3Z2G"
      },
      "source": [
        "## a) **perform MH within Gibbs on the blocks w and t̂**\n",
        "\n",
        "Doing Metropolis-Hastings within Gibbs means that we use MH to sample from the conditionals. In b, we derived the analysticlal forms, and thus we could use them to sample from. However, if we don't have a nice form, we can use MH. If we further more assume that the proposal q is symmetric, ie $q(x|x^*)=q(x^*|x)$, then the MH acceptance probability is even simpler, but we don't have to assume this to do MH in Gibbs.\n",
        "\n",
        "$$\n",
        "A(x^*,x) = \\min\\big(1, \\frac{p(x^*)q(x|x^*)}{p(x)q(x^*|x)}\\big)\n",
        "$$\n",
        "\n",
        "The probability we are interested in estimating here, p(x) is actually the conditional of $\\hat t$ or $w$, so we have two acceptance probabilities with\n",
        "  * $x = \\hat t| w,\\hat x, x, t, \\sigma^2,\\alpha$ \n",
        "  * and $x^* = \\hat t^*| w,\\hat x, x, t, \\sigma^2,\\alpha$\n",
        "\n",
        "for estimating $p(\\hat t| w,\\hat x, x, t, \\sigma^2,\\alpha)$\n",
        "\n",
        "and\n",
        "  * $x = w|\\hat t^* ,\\hat x, x, t, \\sigma^2,\\alpha$ \n",
        "  * and $x^* = w^*| \\hat t^*,\\hat x, x, t, \\sigma^2,\\alpha$\n",
        "\n",
        "for estimating $p(w| \\hat t^*,\\hat x, x, t, \\sigma^2,\\alpha)$\n",
        "\n",
        "Note that since the conditional is proportional to the joint, up to a normalization constant (the evidence, that cancels out), we can use it in place of the conditional. This is very convenient when getting the evidence is difficult.\n",
        "\n",
        "$$\n",
        "\\frac{p(x^*)}{p(x)} = \\frac{p(\\hat t^*| w,\\hat x, x, t, \\sigma^2,\\alpha)}{p(\\hat t| w,\\hat x, x, t, \\sigma^2,\\alpha)} = \\frac{p(\\hat t^*, w,\\hat x, x, t, \\sigma^2,\\alpha)/p(w,\\hat x, x, t, \\sigma^2,\\alpha)}{p(\\hat t, w,\\hat x, x, t, \\sigma^2,\\alpha)/p(w,\\hat x, x, t, \\sigma^2,\\alpha)} \n",
        "\\\\\n",
        "=\\frac{p(\\hat t^*, w,\\hat x, x, t, \\sigma^2,\\alpha)}{p(\\hat t, w,\\hat x, x, t, \\sigma^2,\\alpha)}\n",
        "$$\n",
        "\n",
        "The joints facrize nicely, and everything cancels except for the 1D Gaussian of the new data point - even the Gaussian normalization constant cancels\n",
        "\n",
        "$$\n",
        "\\frac{p(x^*)}{p(x)} = \n",
        "\\frac\n",
        "{\\exp\\frac{-(\\hat t^*-w^T\\hat x)^2}{2\\sigma^2}}\n",
        "{\\exp\\frac{-(\\hat t-w^T\\hat x)^2}{2\\sigma^2}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YceAemitBCxI"
      },
      "source": [
        "Likewise, for updating w (taking care to use the new $\\hat t^*$ we get\n",
        "\n",
        "$$\n",
        "\\frac{p(x^*)}{p(x)} \n",
        "= \\frac{p(\\hat t^*, w^*,\\hat x, x, t, \\sigma^2,\\alpha)}{p(\\hat t^*, w,\\hat x, x, t, \\sigma^2,\\alpha)}\n",
        "= \\frac{p(\\hat t^*| w^*,\\hat x, \\sigma^2)p(w^*|\\alpha)}{p(\\hat t^*| w,\\hat x, \\sigma^2)p(w^*|\\alpha)}\n",
        "\\\\\n",
        "\\frac\n",
        "{\\exp\\frac{-(\\hat t^*-{w^{*}}^T\\hat x)^2}{2\\sigma^2} - \\frac{{w^{*}}^Tw^{*}}{2\\alpha}}\n",
        "{\\exp\\frac{-(\\hat t^*-w^T\\hat x)^2}{2\\sigma^2} - \\frac{w^Tw}{2\\alpha}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN9Xy9lN8P1u"
      },
      "source": [
        "# Question 5\n",
        "**Implement a sampler for the LDA model on a corpus consisting of abstracts from NeurIPS in years past. You may wish to start from support code that will be provided on the course website. Use the output to answer questions about the NIPS abstract corpus. The data for this model are in bagofwords_nips.mat, words_nips.mat, and title_nips.mat. When you load these files in the variables DS, WS, WO, and titles will appear in your workspace. The variable WS is the entire corpus vectorized into a long row of words encoded as a row vector of integers. Each integer represents a word, WO is the dictionary; to look up the word corresponding to an integer use WOi. The variable DS is a row vector of the same length as WS that indicates from which document each word comes. If the document number is j then the title of that document can be found using titles[j]. Train LDA with 20 topics using an MCMC sampler in the collapsed representation (i.e. with all βk’s and θd’s integrated out analytically) and use the single sample with the highest joint log likelihood to answer the following questions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1sBToPiadxV"
      },
      "source": [
        "To answer this problem, we need to code up the function for the joint probability, and for sampling.\n",
        "\n",
        "We use the notation of https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Aspects_of_computational_details, except that $\\beta$ is written as $\\gamma$, following the starter code.\n",
        "\n",
        "The joint probability p(Z,W) assigns a number to each instantiation of all the words having some assigned topic (Z), and also the identity of each word being what it is (W). Note that in our example, W is fixed and doesn't change, since each word is what it is. However, we can still discuss its probability because of assumptions we make in the model. For instance, having a corpus that is all the same word is less likely that having one with words that are more evenly distributed, because in we are going to use a the same prior parameter $\\gamma$ for every word.\n",
        "\n",
        "Based on the probabilistic graphical model we can write down the joint as factorizing as factorizing into \n",
        "* priors over $\\alpha$ and $\\gamma$ \n",
        "* and the conditional probabilities of\n",
        "  * $Z_{jt}|\\theta_j$: (the topic of) word $t$ in document $j$ being a certain topic, given a certain topic distribution vector in document j\n",
        "  *  $W_{jt}|\\phi_{Z_{jt}}$: word $t$ in document $j$ being a certain word, given a certain word distribution vector for topic $Z_{jt}$, i.e. $\\phi_{Z_{jt}}$ is a distribution over the whole corpus, for topic $Z_{jt}$, over words\n",
        "\n",
        "Since the we commit to a Dirichlet distribution for $\\theta,\\phi$ in the model, we can integrate them out of the joint, and this is tractable analytically.\n",
        "\n",
        "Because of the graph (conditional probability of $Z_{jt}$ are dependent on $\\theta_j$ and not $\\phi_{Z_{jt}}$; and likewise $W_{jt}$ not dependent on $\\theta_j$), these integrals split up nicely into a piece over $\\theta$ and another over $\\phi$. \n",
        "\n",
        "Considering the sub-piece of $\\theta_j$ for document $j$ we have\n",
        "$$\n",
        "\\int_{\\theta_j} d\\theta_j \\ p(\\theta_j ; \\alpha) \\prod_{t=1}^{N_j} p(Z_{jt}|\\theta_j) \n",
        "\\\\\n",
        "= \\int_{\\theta_j} \\frac{\\Gamma(\\sum_i^K \\alpha_i)}{\\prod_i^K\\Gamma(\\alpha_i)}\\prod\\theta_{ji}^{\\alpha_i-1}\\prod_t^{N_j}p(Z_{jt}|\\theta_j)\n",
        "$$\n",
        "\n",
        "A key step in occurs here, where we relate this probability to the counts of words and topics, which we have on hand and can conveniently store and update. The key step is to swap the product sum over the words $t$ in document $j$ with a product sum over topics $i$ of the counts of all words belonging top topic i in document j, denoted by $n_{j*}^i$. The counts in the exponents of $\\theta_j$ of the categorical distribution of $Z$ sum up, and can now be combined with the $\\alpha_i-1$ in the exponent from the prior. The functional form is Dirichlet, with parameter $n_{j*}^i + \\alpha_i$ with no mixing between $\\theta_j$s, and we know what the integral evaluates to because we know the normalization constant for the Dirichlet distribution. The outer product sum over all the documents gives us the whole theta piece as\n",
        "\n",
        "$$\n",
        "\\prod_j^M \n",
        "\\frac{\\Gamma(\\sum_i^K \\alpha_i)}{\\prod_i^K\\Gamma(\\alpha_i)} \n",
        "\\frac{\\prod_i^K\\Gamma(n_{j*}^i + \\alpha_i)}{\\Gamma(\\sum_i^K n_{j*}^i + \\alpha_i)}\n",
        "$$\n",
        "\n",
        "A similar argument holds for $\\phi,\\gamma$ piece, the sum happening over the same words in all documents, $n_{*r}^i$\n",
        "  * $\\theta_j \\rightarrow \\phi_i$\n",
        "  * $\\alpha_i \\rightarrow \\gamma_r$\n",
        "  * $n_{j*}^i \\rightarrow n_{*r}^i$\n",
        "  * $\\prod_j^M \\rightarrow \\prod_i^K$\n",
        "  * $\\sum_i^K / \\prod_i^K \\rightarrow \\sum_r^V  / \\prod_r^V$\n",
        "\n",
        "And thus the joint with $\\theta,\\phi$ analytically integrated out is\n",
        "\n",
        "$$\n",
        "p(Z,W;\\alpha,\\gamma) = \n",
        "\\prod_j^M \n",
        "\\frac{\\Gamma(\\sum_i^K \\alpha_i)}{\\prod_i^K\\Gamma(\\alpha_i)} \n",
        "\\frac{\\prod_i^K\\Gamma(n_{j*}^i + \\alpha_i)}{\\Gamma(\\sum_i^K n_{j*}^i + \\alpha_i)}\n",
        "\\prod_i^K\n",
        "\\frac{\\Gamma(\\sum_r^V \\gamma_r)}{\\prod_r^V\\Gamma(\\gamma_r)} \n",
        "\\frac{\\prod_r^V\\Gamma(n_{*r}^i + \\gamma_r)}{\\Gamma(\\sum_r^V n_{*r}^i + \\beta_r)}\n",
        "$$\n",
        "\n",
        "Furthermore, if we assume the $\\alpha$s and $\\gamma$s are the same for all topics and words, respectively, then we can factor them out of the sums. When we take the log of this, the product sums become products. These are very large numbers and we get overlow at $\\Gamma(\\geq 173)$. However the $\\log$ of this would be much smaller, and a $log \\Gamma$ function is implemented in `scipy.special.loggamma` that allows us to avoid numerical overflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5rtCzTccJs4"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "import scipy.special\n",
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knotQHBy3vdH"
      },
      "source": [
        "!wget https://www.cs.ubc.ca/~fwood/CS532W-539W/homeworks/data/bagofwords_nips.mat\n",
        "!wget https://www.cs.ubc.ca/~fwood/CS532W-539W/homeworks/data/words_nips.mat\n",
        "!wget https://www.cs.ubc.ca/~fwood/CS532W-539W/homeworks/data/titles_nips.mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiOfztx55ZGP"
      },
      "source": [
        "bagofwords = scipy.io.loadmat('bagofwords_nips.mat')\n",
        "words_d = scipy.io.loadmat('words_nips.mat')\n",
        "titles = scipy.io.loadmat('titles_nips.mat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2hbpnOeqEuy"
      },
      "source": [
        "titles['titles'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIOAQfnK8DG7"
      },
      "source": [
        "bagofwords['WS'].shape # The variable WS is the entire corpus vectorized into a long row of words encoded as a row vector of integers."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJoCGjmAAfl-"
      },
      "source": [
        "t = 2301375-1 # there are N=2301375 number of total words in all documents \n",
        "bagofwords['WS'][0,t] # these are the integer keys of each word, that can be used to look up the string word in words['WO']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idf3Q1QQDZiv"
      },
      "source": [
        "np.max(bagofwords['DS']), bagofwords['DS'].shape # The variable DS is a row vector of the same length as WS that indicates from which document each word comes. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohoe9oKf8YbN"
      },
      "source": [
        "i=13649-1 # there are 13649 different words\n",
        "len(words_d['WO']), words_d['WO'][i] # Each integer represents a word, WO is the dictionary; to look up the word corresponding to an integer use WOi."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMhGcHr4aRc6"
      },
      "source": [
        "words_d['WO']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhqx2aXt8qqs"
      },
      "source": [
        "j=1740-1 # there are M=1740 documents\n",
        "titles['titles'].shape,titles['titles'][j] # If the document number is j then the title of that document can be found using titles[j]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu18eQKh871y"
      },
      "source": [
        "Train LDA with 20 topics using an MCMC sampler in the collapsed representation (i.e. with all βk’s and θd’s integrated out analytically) and use the single sample with the highest joint log likelihood to answer the following questions :\n",
        "\n",
        "  * What are the top ten most probable words for each of the 20 topics?\n",
        "  * One can consider the distribution over topics as a low dimensional representation of a document. We can use the dot product between topic distributions for two documents as a similarity metric. What are the ten most similar documents to the first document, “Connectivity Versus Entropy”?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUyawr7CCjzh"
      },
      "source": [
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from joint_log_lik import joint_log_lik\n",
        "# from sample_topic_assignment import sample_topic_assignment\n",
        "\n",
        "bagofwords = loadmat('bagofwords_nips.mat')\n",
        "WS = bagofwords['WS'][0] - 1  #go to 0 indexed\n",
        "DS = bagofwords['DS'][0] - 1\n",
        "\n",
        "WO = loadmat('words_nips.mat')['WO'][:,0]\n",
        "titles = loadmat('titles_nips.mat')['titles'][:,0]\n",
        "\n",
        "#This script outlines how you might create a MCMC sampler for the LDA model\n",
        "\n",
        "alphabet_size = WO.size\n",
        "\n",
        "document_assignment = DS\n",
        "words = WS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdvnWmmycf0E",
        "outputId": "0fe53f9a-51a5-42f9-d6ea-0ac28af7f964"
      },
      "source": [
        "document_assignment.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2301375,)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGJH7Fo-CmZX"
      },
      "source": [
        "#subset data, EDIT THIS PART ONCE YOU ARE CONFIDENT THE MODEL IS WORKING\n",
        "#PROPERLY IN ORDER TO USE THE ENTIRE DATA SET\n",
        "n_docs = np.unique(document_assignment).size\n",
        "idx_subset = document_assignment < n_docs # subste of 100 documents\n",
        "words = words[idx_subset]\n",
        "document_assignment  = document_assignment[idx_subset]\n",
        "\n",
        "assert np.isclose(n_docs, document_assignment.max() + 1)\n",
        "\n",
        "#number of topics\n",
        "n_topics = 20\n",
        "\n",
        "#initial topic assigments\n",
        "np.random.seed(10)\n",
        "topic_assignment = np.random.randint(n_topics, size=document_assignment.size) # integer topic assignment of 149771 words\n",
        "\n",
        "#within document count of topics\n",
        "doc_counts = np.zeros((n_docs,n_topics))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BP8hoHjCtBL"
      },
      "source": [
        "for d in range(n_docs):\n",
        "    #histogram counts the number of occurences in a certain defined bin\n",
        "    # documents x topics. un-normalized theta\n",
        "    doc_counts[d] = np.histogram(topic_assignment[document_assignment == d], bins=n_topics, range=(-0.5,n_topics-0.5))[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnk048hMEPQb"
      },
      "source": [
        "#doc_N: array of size n_docs count of total words in each document, minus 1\n",
        "doc_N = doc_counts.sum(axis=1) - 1\n",
        "\n",
        "#within topic count of words\n",
        "# topics x words\n",
        "topic_counts = np.zeros((n_topics,alphabet_size))\n",
        "for k in range(n_topics):\n",
        "    w_k = words[topic_assignment == k]\n",
        "    topic_counts[k] = np.histogram(w_k, bins=alphabet_size, range=(-0.5,alphabet_size-0.5))[0]\n",
        "\n",
        "#topic_N: array of size n_topics count of total words assigned to each topic\n",
        "topic_N = topic_counts.sum(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkXCRROLC78X"
      },
      "source": [
        "See comments in `joint_log_lik` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFBJzWlS9B7Z",
        "outputId": "2b89f5aa-90e5-460a-9217-55fb72f2fde1"
      },
      "source": [
        "def joint_log_lik(doc_counts, topic_counts, alpha, gamma):\n",
        "    \"\"\"\n",
        "    Calculate the joint log likelihood of the model\n",
        "    \n",
        "    Args:\n",
        "        doc_counts: n_docs x n_topics array of counts per document of unique topics\n",
        "        topic_counts: n_topics x alphabet_size array of counts per topic of unique words\n",
        "        alpha: prior dirichlet parameter on document specific distributions over topics\n",
        "        gamma: prior dirichlet parameter on topic specific distribuitons over words.\n",
        "    Returns:\n",
        "        ll: the joint log likelihood of the model\n",
        "    \"\"\"\n",
        "\n",
        "    # note that scipy.special.gamma goes until 171, 172 is infinity, so use loggamma instead\n",
        "    # see https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Aspects_of_computational_details\n",
        "      # in particular, the joint p(zw) with theta, phi marginalized out\n",
        "      # https://wikimedia.org/api/rest_v1/media/math/render/svg/fddec8ee49ae29b995974494e5cf11fe5709bafb\n",
        "    theta = doc_counts # (100, 20) (M,K) doc_counts[m,k] is number of times document m uses topic k\n",
        "    phi = topic_counts # (20, 13649) (K,V) topic_counts[k,v] is number of times topic k uses word v\n",
        "    \n",
        "    K = n_topics\n",
        "    M = n_docs\n",
        "    K,V = phi.shape\n",
        "    M,K_ = theta.shape\n",
        "    assert K == K_\n",
        "\n",
        "    # numerator theta / alpha\n",
        "    #loggamma_sum_alpha = M*scipy.special.loggamma(K*alpha) # constant can drop\n",
        "    n_alpha = doc_counts + alpha # n^{i}_{j,(\\cdot)}\n",
        "    loggamma_n_alpha = (scipy.special.loggamma(n_alpha)).sum()\n",
        "    # denomenator theta / alpha\n",
        "    #loggamma_alpha = M*K*scipy.special.loggamma(alpha)\n",
        "    loggamma_sum_n_alpha = scipy.special.loggamma(n_alpha.sum(1)).sum()\n",
        "\n",
        "    # numerator phi / gamma\n",
        "    #loggamma_sum_gamma = K*scipy.special.loggamma(V*gamma)\n",
        "    n_gamma = topic_counts + gamma # n^{i}_{(\\cdot),r}\n",
        "    loggamma_n_gamma = (scipy.special.loggamma(n_gamma)).sum()\n",
        "    # denomenator phi / gamma\n",
        "    #loggamma_gamma = K*V*scipy.special.loggamma(gamma)\n",
        "    loggamma_sum_n_gamma = scipy.special.loggamma(n_gamma.sum(1)).sum()\n",
        "\n",
        "    ll =  + loggamma_n_alpha - loggamma_sum_n_alpha \\\n",
        "    + loggamma_n_gamma  - loggamma_sum_n_gamma\n",
        "\n",
        "    return ll\n",
        "\n",
        "%time joint_log_lik(doc_counts, topic_counts, alpha=0.1, gamma=0.001)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 15.9 ms, sys: 0 ns, total: 15.9 ms\n",
            "Wall time: 18.6 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "-24586180.790821318"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlLMErHo9KhX"
      },
      "source": [
        "def sample_topic_assignment(topic_assignment,\n",
        "                            topic_counts,\n",
        "                            doc_counts,\n",
        "                            topic_N,\n",
        "                            doc_N,\n",
        "                            alpha,\n",
        "                            gamma,\n",
        "                            words,\n",
        "                            document_assignment,\n",
        "                            sampled_word_idx,\n",
        "                            topic_integers,\n",
        "                            n_topics,\n",
        "                            alphabet_size):\n",
        "    \"\"\"\n",
        "    Sample the topic assignment for each word in the corpus, one at a time.\n",
        "    \n",
        "    Args:\n",
        "        topic_assignment: size n array of topic assignments\n",
        "        topic_counts: n_topics x alphabet_size array of counts per topic of unique words        \n",
        "        doc_counts: n_docs x n_topics array of counts per document of unique topics\n",
        "\n",
        "        topic_N: array of size n_topics count of total words assigned to each topic\n",
        "        doc_N: array of size n_docs count of total words in each document, minus 1\n",
        "        \n",
        "        alpha: prior dirichlet parameter on document specific distributions over topics\n",
        "        gamma: prior dirichlet parameter on topic specific distribuitons over words.\n",
        "\n",
        "        words: size n array of wors\n",
        "        document_assignment: size n array of assignments of words to documents\n",
        "    Returns:\n",
        "        topic_assignment: updated topic_assignment array\n",
        "        topic_counts: updated topic counts array\n",
        "        doc_counts: updated doc_counts array\n",
        "        topic_N: updated count of words assigned to each topic\n",
        "    \"\"\"\n",
        "    # sampled_word_idx = 0\n",
        "    document_idx = document_assignment[sampled_word_idx]\n",
        "    word_idx = words[sampled_word_idx]\n",
        "    old_topic_idx = topic_assignment[sampled_word_idx]\n",
        "    \n",
        "    # remove sampled from counts\n",
        "    # TODO check how sparse and if sparse encoding helps\n",
        "    topic_counts[old_topic_idx,word_idx] -= 1\n",
        "    doc_counts[document_idx,old_topic_idx] -= 1\n",
        "    topic_N[old_topic_idx] -= 1\n",
        "\n",
        "    # using notation from\n",
        "    # https://medium.com/analytics-vidhya/topic-modeling-using-lda-and-gibbs-sampling-explained-49d49b3d1045\n",
        "    # n = doc_counts\n",
        "    # v = topic_counts\n",
        "    # lam = gamma\n",
        "    # z = topic_assignment\n",
        "    # n_alpha = n+alpha\n",
        "    # v_lam = v+lam\n",
        "\n",
        "    # unvectorized\n",
        "    # p = (n_alpha[document_idx,k])*(v_lam[k,w]) / (n_alpha[document_idx].sum()*v_lam[k].sum()) \n",
        "    # vectorize over k\n",
        "    # p = n_alpha[document_idx]*v_lam[:,word_idx] / (n_alpha[document_idx].sum()*v_lam.sum(1))\n",
        "\n",
        "    # Eq 27.37 in p. 956 of Murphy, 2012\n",
        "    K = n_topics\n",
        "    V = alphabet_size\n",
        "    c_vk = topic_counts\n",
        "    c_ik = doc_counts\n",
        "    c_k = topic_N\n",
        "    L_i = doc_N\n",
        "\n",
        "    p = (c_ik[document_idx] + alpha)*(c_vk[:,word_idx] + gamma) / ((c_k + V*gamma)*(L_i[document_idx]+K*alpha))\n",
        "    \n",
        "    z_sampled = np.random.choice(topic_integers,size=1,p=p/p.sum())\n",
        "    new_topic_idx = z_sampled\n",
        "\n",
        "    # update\n",
        "    topic_counts[new_topic_idx,word_idx] += 1\n",
        "    doc_counts[document_idx,new_topic_idx] += 1\n",
        "    topic_assignment[sampled_word_idx] = new_topic_idx\n",
        "    #topic_N = topic_counts.sum(axis=1) \n",
        "    topic_N[new_topic_idx] += 1\n",
        "    \n",
        "\n",
        "    return topic_assignment, topic_counts, doc_counts, topic_N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_A1Zip4h382"
      },
      "source": [
        "#prior parameters, alpha parameterizes the dirichlet to regularize the\n",
        "#document specific distributions over topics and gamma parameterizes the \n",
        "#dirichlet to regularize the topic specific distributions over words.\n",
        "#These parameters are both scalars and really we use alpha * ones() to\n",
        "#parameterize each dirichlet distribution. Iters will set the number of\n",
        "#times your sampler will iterate.\n",
        "alpha = 0.1\n",
        "gamma = 0.001 \n",
        "iters = 150\n",
        "\n",
        "jll = []\n",
        "\n",
        "ll_max = joint_log_lik(doc_counts,topic_counts,alpha,gamma)\n",
        "jll.append(ll_max)\n",
        "\n",
        "n_words = words.size\n",
        "topic_integers = np.arange(20)\n",
        "iter_li = []\n",
        "    \n",
        "K = n_topics\n",
        "M = n_docs\n",
        "K,V = topic_counts.shape # (20, 13649) (K,V) topic_counts[k,v] is number of times topic k uses word v\n",
        "M,K_ = doc_counts.shape # (100, 20) (M,K) doc_counts[m,k] is number of times document m uses topic k\n",
        "assert K == K_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYVqwlDrcZVD",
        "outputId": "3b0ae809-b5f9-4217-c26d-4063896d22e4"
      },
      "source": [
        "%%timeit \n",
        "sampled_word_idx=0\n",
        "sample_topic_assignment(\n",
        "                        topic_assignment,\n",
        "                        topic_counts,\n",
        "                        doc_counts,\n",
        "                        topic_N,\n",
        "                        doc_N,\n",
        "                        alpha,\n",
        "                        gamma,\n",
        "                        words,\n",
        "                        document_assignment,\n",
        "                        sampled_word_idx,\n",
        "                        topic_integers,\n",
        "                        K,\n",
        "                        V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The slowest run took 10.13 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10000 loops, best of 5: 72.4 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-MVgu_SZ9Nde"
      },
      "source": [
        "for iter in range(iters):\n",
        "  #print('iteration',iter)\n",
        "  sampled_word_idxs = np.random.permutation(words.size)\n",
        "  for i,sampled_word_idx in enumerate(sampled_word_idxs):\n",
        "    if (i % (n_words//10)) == 0: print(i)\n",
        "    # prm = np.random.permutation(words.shape[0])\n",
        "    # words = words[prm]   \n",
        "    # document_assignment = document_assignment[prm]\n",
        "    # topic_assignment = topic_assignment[prm]\n",
        "\n",
        "    #sampled_word_idx = np.random.choice(n_words)\n",
        "    \n",
        "    topic_assignment, topic_counts, doc_counts, topic_N = sample_topic_assignment(\n",
        "                                topic_assignment,\n",
        "                                topic_counts,\n",
        "                                doc_counts,\n",
        "                                topic_N,\n",
        "                                doc_N,\n",
        "                                alpha,\n",
        "                                gamma,\n",
        "                                words,\n",
        "                                document_assignment,\n",
        "                                sampled_word_idx,\n",
        "                                topic_integers,\n",
        "                                K,\n",
        "                                V)\n",
        "    if (i % (n_words//1000)) == 0:\n",
        "      #print(i,end=' ')\n",
        "      ll = joint_log_lik(doc_counts,topic_counts,alpha,gamma)\n",
        "      jll.append(ll)\n",
        "      iter_li.append(iter)\n",
        "  ll = joint_log_lik(doc_counts,topic_counts,alpha,gamma)\n",
        "  jll.append(ll)\n",
        "  iter_li.append(iter)\n",
        "  if ll > ll_max:\n",
        "    ll_max = ll\n",
        "    topic_counts_max = topic_counts\n",
        "plt.plot(jll)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOgZekCD-UfP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "970b9706-69ac-4fdd-8507-3031afa0d1aa"
      },
      "source": [
        "sr = pd.Series(jll)\n",
        "sr.index = np.linspace(0,np.max(iter_li),len(jll))\n",
        "sr.plot()\n",
        "plt.ylabel('Log p(Z,W)')\n",
        "plt.xlabel('Iteration (loop through corpus)')\n",
        "plt.title('Joint distribution')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Joint distribution')"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhGBM_gi_Fo6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "37e53d60-1920-451a-e8b1-f7d48b47e915"
      },
      "source": [
        "sr = pd.Series(jll)\n",
        "sr.index = np.linspace(0,np.max(iter_li),len(jll))\n",
        "sr.iloc[-4000:].plot()\n",
        "plt.ylabel('Log p(Z,W)')\n",
        "plt.xlabel('Iteration (loop through corpus)')\n",
        "plt.title('Joint distribution (at end of run showing equilibrium)\\n')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Joint distribution (at end of run showing equilibrium)\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN9X6O8TUA2e"
      },
      "source": [
        "import pickle\n",
        "wdir = '/content/drive/MyDrive/ubc_cs/2021w/cpsc532w'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh3WszarUxl0",
        "outputId": "0366f364-8911-4c7b-eba4-56cced5ecb30"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btDusOgiRwlS"
      },
      "source": [
        "import time\n",
        "fname = ('').join([wdir,'/hw1-q5-',time.strftime('%Y%m%d-%H%M'),'.p'])\n",
        "\n",
        "data_d = {'topic_assignment':topic_assignment,\n",
        "          'topic_counts':topic_counts, \n",
        "          'doc_counts':doc_counts,\n",
        "          'topic_N':topic_N,\n",
        "          'alpha':alpha,\n",
        "          'gamma':gamma,\n",
        "          'n_topics':n_topics,\n",
        "          'alphabet_size':alphabet_size,\n",
        "          'n_words':n_words,\n",
        "          'll_max':ll_max,\n",
        "          'n_docs':n_docs,\n",
        "          'jll':jll,\n",
        "          'iter':iter_li,\n",
        "}\n",
        "\n",
        "pickle.dump( data_d, open( fname, \"wb\" ) )"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AJiCngLYNzq",
        "outputId": "44f8c9f5-12e4-4d6c-c4bd-33d78a10a99c"
      },
      "source": [
        "!ls /content/drive/MyDrive/ubc_cs/2021w/cpsc532w"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hw1-q5-20210928-1708.p\thw1-q5-20210930-1940.p\thw1-q5-20211001-0233.p\n",
            "hw1-q5-20210929-0530.p\thw1-q5-20210930-2348.p\thw1-q5-20211001-0525.p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCatMzHBSPdx"
      },
      "source": [
        "# fname_load = '/content/drive/MyDrive/ubc_cs/2021w/cpsc532w/hw1-q5-20210929-0530.p'\n",
        "# load_d = pickle.load( open( fname_load, \"rb\" ) )\n",
        "\n",
        "# alpha = load_d['alpha']\n",
        "# topic_assignment=load_d['topic_assignment']\n",
        "# topic_counts=load_d['topic_counts']\n",
        "# doc_counts=load_d['doc_counts']\n",
        "# topic_N=load_d['topic_N']\n",
        "# alpha=load_d['alpha']\n",
        "# gamma=load_d['gamma']\n",
        "# n_topics=load_d['n_topics']\n",
        "# alphabet_size=load_d['alphabet_size']\n",
        "# n_words=load_d['n_words']\n",
        "# ll_max=load_d['ll_max']\n",
        "# n_docs =load_d['n_docs']\n",
        "# jll=load_d['jll']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM4Z-IPrDXDY"
      },
      "source": [
        "## **What are the top ten most probable words for each of the 20 topics?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0md0NWEWEhh_"
      },
      "source": [
        "### find the 10 most probable words of the 20 topics:\n",
        "ind = np.argsort(topic_counts_max, axis=1) # note that this is from lest to most probable, and we should look at the end of the array for the popular"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SWjZrjLEpXy",
        "outputId": "6224b96e-d6d8-45f8-feca-af8b4b109a56"
      },
      "source": [
        "n_prob_words = 10\n",
        "most_popular_words = words_d['WO'][ind[:,-n_prob_words:].flatten()].reshape(n_topics,n_prob_words)\n",
        "print('most popular words')\n",
        "for row in range(most_popular_words.shape[0]):\n",
        "  print('topic %i'%row,end=\" \")\n",
        "  for col in range(most_popular_words.shape[1]):\n",
        "    print(most_popular_words[row,col][0],end=\" \")\n",
        "  print()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "most popular words\n",
            "topic 0 face pixel objects figure feature features recognition object images image \n",
            "topic 1 linear distribution mixture space matrix models algorithm gaussian model data \n",
            "topic 2 fixed model recurrent states neural phase dynamics system state time \n",
            "topic 3 function functions probability class bound error size examples set number \n",
            "topic 4 analysis order frequency filter processing signals time noise information signal \n",
            "topic 5 connectionist graph level representations rule representation set tree rules structure \n",
            "topic 6 vector classifier error class test performance classification data set training \n",
            "topic 7 direction eye cells orientation cortex response stimulus motion model visual \n",
            "topic 8 approximation matrix gradient case problem functions algorithm vector linear function \n",
            "topic 9 vlsi input neuron voltage output current figure chip circuit analog \n",
            "topic 10 number output net nodes layer memory input networks neural network \n",
            "topic 11 neural log variables prior bayesian parameters models distribution probability model \n",
            "topic 12 rate model input spike firing cells synaptic neuron cell neurons \n",
            "topic 13 unit weights networks output error hidden input training units network \n",
            "topic 14 regions target location space region fig position fields map field \n",
            "topic 15 control time actions function optimal states policy reinforcement action state \n",
            "topic 16 words time hmm training context sequence system word recognition speech \n",
            "topic 17 search learned number learn problem task algorithms time algorithm learning \n",
            "topic 18 figure systems controller feedback forward neural motor control system model \n",
            "topic 19 kernel prediction results regression estimate approach based methods method data \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T15-uCGj1ua"
      },
      "source": [
        "**One can consider the distribution over topics as a low dimensional representation of a document. We can use the dot product between topic distributions for two documents as a similarity metric. \n",
        "\n",
        "## **What are the ten most similar documents to the first document, “Connectivity Versus Entropy”?**\n",
        "\n",
        "* Normalize counts to distribution (sum to 1). Use cosine similarity to handle confounding length/size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNhEWDB2jyGd"
      },
      "source": [
        "query_idx = 0\n",
        "topics_dist = doc_counts/doc_counts.sum(1).reshape(-1,1)\n",
        "def norm(arr,axis=None): \n",
        "  if axis is None:\n",
        "    return np.sqrt((arr*arr).sum())\n",
        "  else:\n",
        "    return np.sqrt((arr*arr).sum(axis))\n",
        "similarity = topics_dist.dot(topics_dist[query_idx,:]) / (norm(topics_dist,1) * norm(topics_dist[query_idx,:]))"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f5gGlknZpxF",
        "outputId": "9961e930-9b46-4201-de4b-2482defe2270"
      },
      "source": [
        "n_sim=10\n",
        "similar_idx = np.argsort(similarity)[-n_sim:][::-1]\n",
        "for idx, close_topic in enumerate([x[0] for x in titles[similar_idx]]):\n",
        "  print(idx,close_topic)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Connectivity Versus Entropy \n",
            "1 LEARNING STOCHASTIC PERCEPTRONS UNDER K-BLOCKING DISTRIBUTIONS \n",
            "2 A Method for Learning from Hints \n",
            "3 The Devil and the Network . \n",
            "4 Strong Unimodality and Exact Learning of Constant Depth g-Perceptron Networks \n",
            "5 On Neural Networks with Minimal Weights \n",
            "6 Estimating Average-Case Learning Curves Using Bayesian, Statistical Physics and VC Dimension Methods \n",
            "7 The VC-Dimension versus the Statistical Capacity of Multilayer Networks \n",
            "8 The Perceptron Algorithm Is Fast for Non-Malicious Distributions \n",
            "9 Neural Computing with Small Weights \n"
          ]
        }
      ]
    }
  ]
}