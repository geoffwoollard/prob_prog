{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe19f28e",
   "metadata": {},
   "source": [
    "## HW3\n",
    "Geoffrey Woollard\n",
    "\n",
    "My code lives in the repo https://github.com/geoffwoollard/prob_prog\n",
    "\n",
    "## Acknowledgments\n",
    "I acknowledge helpful discussions with Justice Sefas, Masoud Mokhatari, Dylan Green, and Jordan Lovrod, and many other classmates.\n",
    "\n",
    "I gratefully acknowledge helpful code snippets from Masoud Mokhatari, Mohamad Amin Mohamadi, and Dylan Green, in particular during the implementation of Hamiltonian Monte Carlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f9309",
   "metadata": {},
   "source": [
    "# Code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92048cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dill.source import getsource, getsourcelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5abd43",
   "metadata": {},
   "source": [
    "## Importance sampling\n",
    "* I modified the evaluator from hw2 to also return $\\sigma$, which gets accumulated from the `log_prob` of evaluating observes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5fbe6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 def evaluate(e,sigma=0,local_env={},defn_d={},do_log=False,logger_string=''):\n",
      "1     # TODO: get local_env to evaluate values to tensors, not regular floats\n",
      "2     # remember to return evaluate (recursive)\n",
      "3     # everytime we call evaluate, we have to use local_env, otherwise it gets overwritten with the default {}\n",
      "4     # if do_log: logger.info('logger_string {}'.format(logger_string))\n",
      "5     if do_log: logger.info('ls {}'.format(logger_string))\n",
      "6     if do_log: logger.info('e {}, local_env {}, sigma {}'.format(e, local_env, sigma))\n",
      "7 \n",
      "8     # get first expression out of list or list of one\n",
      "9     if not isinstance(e,list) or len(e) == 1:\n",
      "10         if isinstance(e,list):\n",
      "11             e = e[0]\n",
      "12         if isinstance(e,bool):\n",
      "13             if do_log: logger.info('match case number: e {}, sigma {}'.format(e, sigma))\n",
      "14             return torch.tensor(e), sigma\n",
      "15         if isinstance(e, number):\n",
      "16             if do_log: logger.info('match case number: e {}, sigma {}'.format(e, sigma))\n",
      "17             return torch.tensor(float(e)), sigma\n",
      "18         elif isinstance(e,list):\n",
      "19             if do_log: logger.info('match case list: e {}, sigma {}'.format(e, sigma))\n",
      "20             return e, sigma\n",
      "21         elif e in list(primitives_d.keys()):\n",
      "22             if do_log: logger.info('match case primitives_d: e {}, sigma {}'.format(e, sigma))\n",
      "23             return e, sigma\n",
      "24         elif e in list(distributions_d.keys()):\n",
      "25             if do_log: logger.info('match case distributions_d: e {}, sigma {}'.format(e, sigma))\n",
      "26             return e, sigma\n",
      "27         elif torch.is_tensor(e):\n",
      "28             if do_log: logger.info('match case is_tensor: e {}, sigma {}'.format(e, sigma))\n",
      "29             return e, sigma\n",
      "30         elif e in local_env.keys():\n",
      "31             if do_log: logger.info('match case local_env: e {}, sigma {}'.format(e, sigma))\n",
      "32             if do_log: logger.info('match case local_env: local_env[e] {}'.format(local_env[e]))\n",
      "33             return local_env[e], sigma # TODO return evaluate?\n",
      "34         elif e in list(defn_d.keys()):\n",
      "35             if do_log: logger.info('match case defn_d: e {}, sigma {}'.format(e, sigma))\n",
      "36             return e, sigma\n",
      "37         elif isinstance(e,distribution_types):\n",
      "38             if do_log: logger.info('match case distribution: e {}, sigma {}'.format(e,sigma))\n",
      "39             return e, sigma\n",
      "40         else:\n",
      "41             assert False, 'case not matched'\n",
      "42     elif e[0] == 'sample':\n",
      "43         if do_log: logger.info('match case sample: e {}, sigma {}'.format(e,sigma))\n",
      "44         distribution, sigma = evaluate(e[1],sigma,local_env,defn_d,do_log=do_log)\n",
      "45         return distribution.sample(), sigma # match shape in number base case\n",
      "46     elif e[0] == 'observe':\n",
      "47         if do_log: logger.info('match case observe: e {}, sigma {}'.format(e,sigma))\n",
      "48         e1, e2 = e[1:]\n",
      "49         d1, sigma = evaluate(e1,sigma,local_env,defn_d,do_log=do_log)\n",
      "50         c2, sigma = evaluate(e2,sigma,local_env,defn_d,do_log=do_log)\n",
      "51         log_w =score(d1,c2)\n",
      "52         if do_log: logger.info('match case observe: d1 {}, c2 {}, log_w {}, sigma {}'.format(e,d1, c2, log_w, sigma))\n",
      "53         sigma += log_w\n",
      "54         return c2, sigma\n",
      "55     elif e[0] == 'let': \n",
      "56         if do_log: logger.info('match case let: e {}, sigma {}'.format(e, sigma))\n",
      "57         # let [v1 e1] e0\n",
      "58         # here \n",
      "59             # e[0] : \"let\"\n",
      "60             # e[1] : [v1, e1]\n",
      "61             # e[2] : e0\n",
      "62         # evaluates e1 to c1 and binds this value to e0\n",
      "63         # this means we update the context with old context plus {v1:c1}\n",
      "64         c1, sigma = evaluate(e[1][1],sigma,local_env,defn_d,do_log=do_log) # evaluates e1 to c1\n",
      "65         v1 = e[1][0]\n",
      "66         return evaluate(e[2], sigma, local_env = {**local_env,v1:c1},defn_d=defn_d,do_log=do_log)\n",
      "67     elif e[0] == 'if': # if e0 e1 e2\n",
      "68         if do_log: logger.info('match case if: e {}, sigma {}'.format(e, sigma))\n",
      "69         e1 = e[1]\n",
      "70         e2 = e[2]\n",
      "71         e3 = e[3]\n",
      "72         e1_prime, sigma = evaluate(e1,sigma,local_env,defn_d,do_log=do_log)\n",
      "73         if e1_prime:\n",
      "74             return evaluate(e2,sigma,local_env,defn_d,do_log=do_log)\n",
      "75         else:\n",
      "76             return evaluate(e3,sigma,local_env,defn_d,do_log=do_log) \n",
      "77 \n",
      "78     else:\n",
      "79         cs = []\n",
      "80         for ei in e:\n",
      "81             if do_log: logger.info('cycling through expressions: ei {}, sigma {}'.format(ei,sigma))\n",
      "82             c, sigma = evaluate(ei,sigma,local_env,defn_d,do_log=do_log)\n",
      "83             cs.append(c)\n",
      "84         if cs[0] in primitives_d:\n",
      "85             if do_log: logger.info('do case primitives_d: cs0 {}'.format(cs[0]))\n",
      "86             if do_log: logger.info('do case primitives_d: cs1 {}'.format(cs[1:]))\n",
      "87             if do_log: logger.info('do case primitives_d: primitives_d[cs[0]] {}'.format(primitives_d[cs[0]]))\n",
      "88             return primitives_d[cs[0]](cs[1:]), sigma\n",
      "89         elif cs[0] in distributions_d:\n",
      "90             if do_log: logger.info('do case distributions_d: cs0 {}'.format(cs[0]))\n",
      "91             return distributions_d[cs[0]](cs[1:]), sigma\n",
      "92         elif cs[0] in defn_d:\n",
      "93             if do_log: logger.info('do case defn: cs0  {}'.format(cs[0]))\n",
      "94             defn_function_li = defn_d[cs[0]]\n",
      "95             defn_function_args, defn_function_body = defn_function_li\n",
      "96             local_env_update = {key:value for key,value in zip(defn_function_args, cs[1:])}\n",
      "97             if do_log: logger.info('do case defn: update to local_env from defn_d {}'.format(local_env_update))\n",
      "98             return evaluate(defn_function_body,sigma,local_env = {**local_env, **local_env_update},defn_d=defn_d,do_log=do_log)\n",
      "99         else:\n",
      "100             assert False, 'not implemented'\n"
     ]
    }
   ],
   "source": [
    "for line_number, function_line in enumerate(getsourcelines(evaluate)[0]):\n",
    "    print(line_number, function_line,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b32c9",
   "metadata": {},
   "source": [
    "I also wrote my own score function. It handles boolean cases by converting them to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334d5d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 def score(distribution,c):\n",
      "1     \"\"\"Score pytorch distributions with .log_prob, but in a robust way for the type of c\n",
      "2     \"\"\"\n",
      "3     if isinstance(c,bool) or c.type() in ['torch.BoolTensor', 'torch.LongTensor']:\n",
      "4         log_w = distribution.log_prob(c.double())\n",
      "5     else:\n",
      "6         log_w = distribution.log_prob(c)\n",
      "7     return log_w\n"
     ]
    }
   ],
   "source": [
    "from evaluation_based_sampling import score\n",
    "for line_number, function_line in enumerate(getsourcelines(score)[0]):\n",
    "    print(line_number, function_line,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ed5d4",
   "metadata": {},
   "source": [
    "I added a few more distributions and boolean operation primitives, for the problems in this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "357c9e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal :\n",
      "0 def normal(mean_std):\n",
      "1     return two_arg_op_primitive(torch.distributions.Normal,mean_std)\n",
      "\n",
      "beta :\n",
      "0 def beta(alpha_beta):\n",
      "1     return two_arg_op_primitive(torch.distributions.Beta,alpha_beta)\n",
      "\n",
      "exponential :\n",
      "0 def exponential(lam):\n",
      "1     return one_arg_op_primitive(torch.distributions.Exponential,lam)\n",
      "\n",
      "uniform :\n",
      "0 def uniform(low_hi):\n",
      "1     return two_arg_op_primitive(torch.distributions.Uniform,low_hi)\n",
      "\n",
      "discrete :\n",
      "0 def discrete(prob_vector):\n",
      "1     return one_arg_op_primitive(torch.distributions.Categorical,prob_vector)\n",
      "\n",
      "flip :\n",
      "0 def flip(prob):\n",
      "1     return one_arg_op_primitive(torch.distributions.bernoulli.Bernoulli,prob)\n",
      "\n",
      "dirichlet :\n",
      "0 def dirichlet(concentration):\n",
      "1     return one_arg_op_primitive(torch.distributions.dirichlet.Dirichlet,concentration)\n",
      "\n",
      "gamma :\n",
      "0 def gamma(concentration_rate):\n",
      "1     return two_arg_op_primitive(torch.distributions.gamma.Gamma,concentration_rate)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from primitives import distributions_d\n",
    "for key in distributions_d.keys() :\n",
    "    print(key,':')\n",
    "    for line_number, function_line in enumerate(getsourcelines(distributions_d[key])[0]):\n",
    "        print(line_number, function_line,end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c56e60bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and :\n",
      "0 def and_primitive(arg1_arg2):\n",
      "1     return two_arg_op_primitive(torch.logical_and,arg1_arg2)  \n",
      "\n",
      "or :\n",
      "0 def or_primitive(arg1_arg2):\n",
      "1     return two_arg_op_primitive(torch.logical_or,arg1_arg2)  \n",
      "\n",
      "> :\n",
      "0 def gt_primitive(consequent_alternative):\n",
      "1     return two_arg_op_primitive(torch.gt,consequent_alternative)\n",
      "\n",
      "< :\n",
      "0 def lt_primitive(consequent_alternative):\n",
      "1     return two_arg_op_primitive(torch.lt,consequent_alternative)\n",
      "\n",
      ">= :\n",
      "0 def ge_primitive(consequent_alternative):\n",
      "1     return two_arg_op_primitive(torch.ge,consequent_alternative)\n",
      "\n",
      "<= :\n",
      "0 def le_primitive(consequent_alternative):\n",
      "1     return two_arg_op_primitive(torch.le,consequent_alternative)\n",
      "\n",
      "= :\n",
      "0 def eq_primitive(consequent_alternative):\n",
      "1     return two_arg_op_primitive(torch.eq,consequent_alternative)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from primitives import primitives_d\n",
    "for key in ['and','or','>','<','>=','<=','='] :\n",
    "    print(key,':')\n",
    "    for line_number, function_line in enumerate(getsourcelines(primitives_d[key])[0]):\n",
    "        print(line_number, function_line,end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778dd8f",
   "metadata": {},
   "source": [
    "## MH within Gibbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab57ed0",
   "metadata": {},
   "source": [
    "I implented Metropolis-Hastings within Gibbs in the following manner\n",
    "* parse the graph in `mh_gibbs_wrapper`\n",
    "* topologically sort the graph vertices \n",
    "* sample from the joint (ie prior) to initialize all values of the graph\n",
    "* cycle through the graph with `gibbs_step` \n",
    "* accept an update at a specific vertex with `accept`\n",
    "* collect each state after a Gibbs update (all the vertices) \n",
    "* return all the fully specified graphs in `gibbs` for `num_steps`\n",
    "* finally, I evaluate return value (the meaning of the program) for all the sampled graphs with `evaluate_program_return_from_samples_whole_graph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "01a42962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 def mh_gibbs_wrapper(graph,num_steps,do_log=False):\n",
      "1     G = graph[1]\n",
      "2     verteces = G['V']\n",
      "3     A = G['A']\n",
      "4     P = G['P']\n",
      "5     X = set(verteces) - set(G['Y'].keys())\n",
      "6     Y = G['Y']\n",
      "7     Y = {key:evaluate([Y[key]], do_log=do_log)[0] for key in Y.keys()}\n",
      "8 \n",
      "9 \n",
      "10     verteces_topsorted = sample_from_joint_precompute(graph)\n",
      "11     _, local_env = sample_from_joint(graph,verteces_topsorted=verteces_topsorted)\n",
      "12     local_env = {**local_env,**Y}\n",
      "13     local_env_list0 = [local_env]\n",
      "14 \n",
      "15     local_env_list = gibbs(num_steps,local_env,P,A,X,do_log=do_log)\n",
      "16 \n",
      "17     local_env_list = local_env_list0 + local_env_list\n",
      "18 \n",
      "19     return_list, samples_whole_graph = evaluate_program_return_from_samples_whole_graph(graph,local_env_list)\n",
      "20 \n",
      "21     \n",
      "22 \n",
      "23     return local_env_list\n",
      "\n",
      "0 def gibbs_step(local_env,P,A,X_sample_vertices,do_log):\n",
      "1     for vertex in X_sample_vertices:\n",
      "2         link_function = P[vertex]\n",
      "3         e = link_function[1]\n",
      "4 \n",
      "5         distribution, sigma = evaluate(e,local_env=local_env,do_log=do_log)\n",
      "6         local_env_prime = local_env.copy()\n",
      "7         local_env_prime[vertex] = distribution.sample()\n",
      "8 \n",
      "9         alpha = accept(vertex,local_env,local_env_prime,A,P,do_log=do_log)\n",
      "10         u = torch.rand(1)\n",
      "11         if u < alpha:\n",
      "12             local_env = local_env_prime\n",
      "13     return local_env\n",
      "\n",
      "0 def accept(vertex,local_env,local_env_prime,A,P,do_log):\n",
      "1     link_function = P[vertex]\n",
      "2     e = link_function[1]\n",
      "3     d_q, _ = evaluate(e,local_env=local_env,do_log=do_log)\n",
      "4     d_q_prime, _ = evaluate(e,local_env=local_env_prime,do_log=do_log)\n",
      "5     log_a = d_q_prime.log_prob(local_env[vertex]) - d_q.log_prob(local_env_prime[vertex])\n",
      "6     V_x = A[vertex] + [vertex] \n",
      "7     for observed_vertex in V_x:\n",
      "8         d_p_prime = evaluate(P[observed_vertex][1],local_env = local_env_prime,do_log=do_log)[0]\n",
      "9         if do_log: evaluate(1,do_log=do_log,logger_string='d_p_prime {}'.format(d_p_prime))\n",
      "10         \n",
      "11         d_p = evaluate(P[observed_vertex][1],local_env = local_env,do_log=do_log)[0]\n",
      "12         if do_log: evaluate(1,do_log=do_log,logger_string='d_p {}'.format(d_p))\n",
      "13         if do_log: evaluate(1,do_log=do_log,logger_string='local_env_prime {}, observed_vertex {}, local_env_prime[observed_vertex] {}'.format(local_env_prime,observed_vertex,local_env_prime[observed_vertex]))\n",
      "14         log_a += score(d_p_prime,local_env_prime[observed_vertex])\n",
      "15         # log_a += d_p_prime.log_prob(local_env_prime[observed_vertex])\n",
      "16         log_a -= score(d_p,local_env[observed_vertex])\n",
      "17         # log_a -= d_p.log_prob(local_env[observed_vertex])\n",
      "18 \n",
      "19     return torch.exp(log_a)\n",
      "\n",
      "0 def gibbs(num_steps,local_env,P,A,X,do_log):\n",
      "1     local_env_list = []\n",
      "2     for step in range(num_steps):\n",
      "3         local_env = gibbs_step(local_env,P,A,X,do_log=do_log)\n",
      "4         local_env_list.append(local_env)\n",
      "5     return local_env_list\n",
      "\n",
      "0 def evaluate_program_return_from_samples_whole_graph(graph,samples_whole_graph):\n",
      "1     # evaluate samples (on whatever function, here the return of the program) as needed\n",
      "2     e = graph[2]\n",
      "3     # TODO suggest daphne put return as program, so return is ['sample2'] not 'sample2'\n",
      "4     if isinstance(e,str):\n",
      "5         e = [e]\n",
      "6     return_list = []\n",
      "7     for X_s in samples_whole_graph:\n",
      "8         return_s, _ = evaluate(e,local_env = X_s) # TODO: handle defns\n",
      "9         return_list.append(return_s)\n",
      "10     return return_list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mh_gibbs import mh_gibbs_wrapper,gibbs_step,accept,gibbs,evaluate_program_return_from_samples_whole_graph\n",
    "list_of_programs = [mh_gibbs_wrapper,gibbs_step,accept,gibbs,evaluate_program_return_from_samples_whole_graph]\n",
    "\n",
    "for program in list_of_programs:\n",
    "    for line_number, function_line in enumerate(getsourcelines(program)[0]):\n",
    "        print(line_number, function_line,end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3a2a5",
   "metadata": {},
   "source": [
    "## Hamiltonian Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440dc619",
   "metadata": {},
   "source": [
    "I implemented HMC in the following way\n",
    "* I parse the graph for the link functions P, and the samples X and observes Y\n",
    "* I turn on autodiff on the torch.tensor(float): `turn_on_autodiff`\n",
    "* I run HMC algorithm 20 from the textbook: `hmc_algo20`\n",
    "  * inside I use the leapfrog algorithm 19 from the textbook: `leapfrog`\n",
    "    * this relies on computing the gradient of the potential energy with respect to the values of X: `grad_U`. There are important impelentation details with pytorch autodiff, avoiding gradient accumulation\n",
    "    * I also have to add Xt (a dict) and Rt (a vector) with a helper function `add_dict_to_tensor`. I use `X_vertex_names_to_idx_d` to keep track of what key in `X` corresponds to what index of `R`. This is important if the keys change order and `M` is different for different values of `X` (i.e. not proportional to the identity matrix)\n",
    "  * I compute the kinetic and potential energy and the hamiltonian: `compute_K`, `compute_U` (just negative of  `compute_log_joint_prob`) and `compute_H`\n",
    "* After I collect samples from the whole graph, I evaluate the return function on each graph.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "8fc16ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 def hmc_wrapper(graph,num_samples,T=10,epsilon=0.1,M=tensor(1.)):\n",
      "1     #set up X, Y list of verteces\n",
      "2     G = graph[1]\n",
      "3     verteces = ['V']\n",
      "4     Y = G['Y']\n",
      "5     P = G['P']\n",
      "6     \n",
      "7     # evaluate to constants\n",
      "8     Y = {key:evaluate([value])[0] for key,value in Y.items()}\n",
      "9     \n",
      "10     #X = set(vertices) - set(Y.keys())\n",
      "11     #X = sample_from_joint(graph)\n",
      "12     _, X0 = sample_from_joint(graph) # does not include observes\n",
      "13     \n",
      "14     # initialize in dict\n",
      "15     X_vertex_names_to_idx_d = {key:idx for idx,key in enumerate(X0.keys())}\n",
      "16 \n",
      "17     # set up autograd on tensors\n",
      "18     turn_on_autodiff(X0)\n",
      "19     turn_on_autodiff(Y) # TODO: why do we need this?\n",
      "20 \n",
      "21     \n",
      "22     # run HMC algorithm 20 from book\n",
      "23         # inside use leapfrog algorithm 19 from book\n",
      "24         # include kinetic and potential energy functions\n",
      "25         # MC acceptance criteria\n",
      "26     samples_whole_graph = hmc_algo20(X0,num_samples,T,epsilon,M,Y,P,X_vertex_names_to_idx_d)\n",
      "27     \n",
      "28     # evaluate samples (on whatever function, here the return of the program) as needed\n",
      "29     e = graph[2]\n",
      "30     # TODO suggest daphne put return as program, so return is ['sample2'] not 'sample2'\n",
      "31     if isinstance(e,str):\n",
      "32         e = [e]\n",
      "33     return_list = []\n",
      "34     for X_s in samples_whole_graph:\n",
      "35         return_s, _ = evaluate(e,local_env = X_s) # TODO: handle defns\n",
      "36         return_list.append(return_s)\n",
      "37 \n",
      "38     return return_list, samples_whole_graph\n",
      "\n",
      "0 def turn_on_autodiff(dictionary_of_tensors):\n",
      "1     \"\"\"\n",
      "2     cant be integers, ie long tensors, but floats or complex\n",
      "3     \"\"\"\n",
      "4     for x in dictionary_of_tensors.values():\n",
      "5         if torch.is_tensor(x):\n",
      "6             x.requires_grad = True\n",
      "\n",
      "0 def hmc_algo20(X0,num_samples,T,epsilon,M,Y,P,X_vertex_names_to_idx_d):\n",
      "1     X_s = X0\n",
      "2     samples = []\n",
      "3     size = len(X0.keys())\n",
      "4     normal_R_reuse = torch.distributions.Normal(torch.zeros(size),M)\n",
      "5     \n",
      "6     for s in range(num_samples):\n",
      "7         R_s = normal_R_reuse.sample()\n",
      "8         R_p, X_p = leapfrog(copy.deepcopy(X_s),copy.deepcopy(R_s),T,epsilon,Y,P,X_vertex_names_to_idx_d)\n",
      "9         # X_p, R_p = leapfrog(X_s,R_s,T,epsilon,X_vertex_names_to_idx_d)\n",
      "10             # copy X_s?\n",
      "11         u = torch.rand(1)\n",
      "12         delta_H = compute_H(X_p,R_p,M,Y,P) - compute_H(X_p,R_p,M,Y,P)\n",
      "13         boltzmann_ratio = torch.exp(-delta_H)\n",
      "14         if u < boltzmann_ratio:\n",
      "15             X_s = X_p\n",
      "16         #no need to update X_s because should stay the same for next round. \n",
      "17         #X_s turns into X_s_minus from algo 20 by indexing\n",
      "18         samples.append(X_s)\n",
      "19     return samples\n",
      "\n",
      "0 def leapfrog(X0,R0,T,epsilon,Y,P,X_vertex_names_to_idx_d):\n",
      "1     \"\"\"\n",
      "2     leapfrog as in algo 19 of book\n",
      "3     Y and P needed for grad calc\n",
      "4     \"\"\"\n",
      "5     \n",
      "6     epsilon_2 = epsilon/2\n",
      "7     R_t = R0 - epsilon_2 * grad_U(X0,Y,P,X_vertex_names_to_idx_d) \n",
      "8     \n",
      "9     X_t = X0\n",
      "10     for t in range(T-1):\n",
      "11         # TODO: save all in loop instead of overwriting to visualize\n",
      "12         X_t = add_dict_to_tensor(X_t,epsilon*R_t,X_vertex_names_to_idx_d)\n",
      "13         R_t = R_t - epsilon*grad_U(X_t,Y,P,X_vertex_names_to_idx_d)\n",
      "14     X_T = add_dict_to_tensor(X_t,epsilon*R_t,X_vertex_names_to_idx_d)\n",
      "15     R_T = R_t - epsilon_2*grad_U(X_T,Y,P,X_vertex_names_to_idx_d)\n",
      "16     return R_T, X_T\n",
      "\n",
      "0 def grad_U(X,Y,P,X_vertex_names_to_idx_d):\n",
      "1     \"\"\"\n",
      "2     call autodiff backward pass, with constant indexing given by X_vertex_names_to_idx_d\n",
      "3     return vector of gradients\n",
      "4     \"\"\"\n",
      "5     energy_U = compute_U(X,Y,P)\n",
      "6     \n",
      "7     # Zero the gradients. \n",
      "8     # without this running grad_U back to back accumulates the grad (not what we want!)\n",
      "9     for key in X.keys():\n",
      "10         if X[key].grad is not None:\n",
      "11             X[key].grad.zero_()\n",
      "12             \n",
      "13     energy_U.backward()\n",
      "14     grads = torch.zeros(len(X.keys()))\n",
      "15     for key in X.keys():\n",
      "16 #         print('key',key)\n",
      "17         idx = X_vertex_names_to_idx_d[key]\n",
      "18 #         print('idx',idx)\n",
      "19 #         print('X[key]',X[key])\n",
      "20 #         print('X[key].grad',X[key].grad)\n",
      "21         grads[idx] = X[key].grad \n",
      "22             # Need to have been referenced in linking function.\n",
      "23             # So key connected to evaluation of energy.\n",
      "24             # Otherwise key not a part of the computational graph of energy_U, \n",
      "25                 # and grad remains none when run energy_U.backwards()\n",
      "26     return grads\n",
      "\n",
      "0 def add_dict_to_tensor(X,R,X_vertex_names_to_idx_d):\n",
      "1     \"\"\"\n",
      "2     X+R using mapping from X_vertex_names_to_idx_d\n",
      "3     TODO: avoid detach?\n",
      "4     R must be dimension 1, not 0 D\n",
      "5     similar to using with torch.no_grad() as in https://github.com/MasoudMo/cpsc532w_hw/blob/master/HW3/graph_based_sampling.py#L275\n",
      "6     \"\"\"\n",
      "7     assert R.dim() >= 1\n",
      "8     X_new = {}\n",
      "9     for vertex in X.keys():\n",
      "10         idx = X_vertex_names_to_idx_d[vertex]\n",
      "11         # overwriting the value, and only want to autograd accumulated gradient to depend on the final value\n",
      "12         # TODO: would this problem go away if we stored a vector over all leapfrog time steps?\n",
      "13         X_new[vertex] = X[vertex].detach() + R[idx]\n",
      "14         X_new[vertex].requires_grad = True\n",
      "15     return X_new\n",
      "\n",
      "0 def compute_K(R,M):\n",
      "1     R_over_2M = R/(2*M) # TODO: generalize for non scalar M, e.g. diagonal M\n",
      "2     if R.dim() == 0:\n",
      "3         energy_K = R*R_over_2M\n",
      "4     elif R.dim() >= 1:\n",
      "5         energy_K = torch.matmul(R,R_over_2M)\n",
      "6     else:\n",
      "7         assert False\n",
      "8     return energy_K\n",
      "\n",
      "0 def compute_log_joint_prob(X,Y,P):\n",
      "1     \"\"\"\n",
      "2     call link functions under context and score.\n",
      "3     TODO: remove Y dependence\n",
      "4     TODO: add in user defns https://github.com/MasoudMo/cpsc532w_hw/blob/master/HW3/graph_based_sampling.py#L208\n",
      "5     need to parse link functions like \n",
      "6         # 'sample2': ['sample*', ['normal', 1, ['sqrt', 5]]]\n",
      "7         # 'observe3': ['observe*', ['normal', 'sample2', ['sqrt', 2]], 8],\n",
      "8     \"\"\"\n",
      "9     log_prob = tensor(0.0)\n",
      "10     for X_vertex in X.keys():\n",
      "11         e = P[X_vertex][1]\n",
      "12         distribution = evaluate(e,local_env=X)[0]\n",
      "13         log_prob += score(distribution,X[X_vertex])\n",
      "14     for Y_vertex in Y.keys():\n",
      "15         e = P[Y_vertex][1]\n",
      "16         distribution = evaluate(e,local_env=X)[0]\n",
      "17         log_prob += score(distribution,Y[Y_vertex])\n",
      "18     \n",
      "19     return log_prob\n",
      "\n",
      "0 def compute_U(X,Y,P):\n",
      "1     energy_U = -compute_log_joint_prob(X,Y,P)\n",
      "2     return energy_U \n",
      "\n",
      "0 def compute_H(X,R,M,Y,P):\n",
      "1     \"\"\"Compute Hamiltonian.\n",
      "2     \"\"\"\n",
      "3     energy_U = compute_U(X,Y,P)\n",
      "4     energy_K = compute_K(R,M)\n",
      "5     energy_H = energy_U + energy_K\n",
      "6     return energy_H\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hmc import hmc_wrapper,turn_on_autodiff,hmc_algo20,leapfrog,grad_U,add_dict_to_tensor,compute_K,compute_log_joint_prob,compute_U,compute_H\n",
    "\n",
    "list_of_programs = [hmc_wrapper,turn_on_autodiff,hmc_algo20,leapfrog,grad_U,add_dict_to_tensor,compute_K,compute_log_joint_prob,compute_U,compute_H]\n",
    "\n",
    "for program in list_of_programs:\n",
    "    for line_number, function_line in enumerate(getsourcelines(program)[0]):\n",
    "        print(line_number, function_line,end='')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
