{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2af4e6",
   "metadata": {},
   "source": [
    "# HW4\n",
    "Geoffrey Woollard\n",
    "\n",
    "My code lives in the repo https://github.com/geoffwoollard/prob_prog\n",
    "\n",
    "# Acknowledgments\n",
    "I acknowledge helpful discussions with Masoud Mokhatari, Dylan Green, Kevin Yang, Gaurav Bhatt, Ilias Karimalis, Ali Seyfi, Kim Dinh, Alan Milligan, Yuan Tian, and many other classmates on the control variate term in Eq. 4.42 of the course textbook, and for providing ELBOs for comparison.\n",
    "\n",
    "I gratefully acknowledge helpful code snippets from Kevin Yang (UniformContinuous proposal from Gamma), and Kim Dinh for advise on using a global optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a1a92",
   "metadata": {},
   "source": [
    "# Code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb188a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dill.source import getsource, getsourcelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0d700",
   "metadata": {},
   "source": [
    "At a high level `graph_bbvi_algo12` parses the graph, initializes the proposal distributions (using the distributions from Beren's starter code for uncontrained optimization) by sampling from the joint *and returning the distribution objects* (not just sampled values). To do this I redefined the distribution primitives to use the unconstrained optimization distributions.\n",
    "\n",
    "Then I just step through the graph with ancestral sampling, and evaluate each linking function *with a deterministic evaluator*, `eval_algo11_deterministic`. The sample and observe cases are not handled in the evaluator (from algo 11 in the course textbook), but instead in `evaluate_link_function_algo11`, which is very similar to how things were done in `sample_from_joint` for ancestral sampling in previous homeworks.\n",
    "\n",
    "Using `autograd`, I can get the gradients of each sample `t,l` w.r.t the parameters of each proposal distribution. These are collected for a whole minibatch of size L, and then the elbo-gradients function in algorithm 12 uses all the information in the samples, and binds the $b_{d,v}$ terms from Eq. 4.42-4.44 / lines 16,17 in algorithm 12. Note that the textbook is ambigous over the sums in line 16, and is clarified in Eq. 9 of [Ranganath, Gerrish, & Blei (2014). Black box variational inference](https://arxiv.org/pdf/1401.0118.pdf). This is done in `elbo_gradients`. \n",
    "\n",
    "A step is taken in the direction of these stochastic minibatch averaged gradients w.r.t. the log prob (see the `log_prob.backward()` in `grad_log_prob`). This step links to a *global* optimizer that is only initialized once when the distributions are set up (I only once do `.make_copy_with_grads()`) in `optimizer_step`. Note that really autograd is not needed here, I know the analytical forms of the gradients, because I know the analytical form of the proposal distributions. I can even link these analytical gradients back up with an optimizer (or not) and use optimizers in pytorch like `Adam`, `SGD`, etc. with things like weight decay by setting the `.grad` of each parameters to be optimized.\n",
    "\n",
    "The $logW^{t,l}$ are collected and are used for weighting the samples a la importance sampling when computing functions over the returns in the posterior. In principle, I not only have the return for each sample, but the whole sample is defined because I used a graph based sampler. I also keep track of the *best* elbo and use the proposals from this iteration $t_{best}$. Note that this is after the gradients step has been taken in the mini-batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8794980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 import logging\n",
      "1 \n",
      "2 import numpy as np\n",
      "3 import torch\n",
      "4 from torch import tensor\n",
      "5 \n",
      "6 from primitives import primitives_d, distributions_d, number, distribution_types\n",
      "7 import distributions # for unconstrained optimization\n",
      "8 from graph_based_sampling import sample_from_joint, score, topsort\n",
      "9 from distributions import Normal\n",
      "10 \n",
      "11 number = (int,float)\n",
      "12 \n",
      "13 logging.basicConfig(format='%(levelname)s:%(message)s')\n",
      "14 logger = logging.getLogger('simple_example')\n",
      "15 logger.setLevel(logging.DEBUG)\n",
      "16 \n",
      "17 logging.basicConfig(format='%(levelname)s:%(message)s')\n",
      "18 logger_graph = logging.getLogger('simple_example')\n",
      "19 logger_graph.setLevel(logging.DEBUG)\n",
      "20 \n",
      "21 def eval_algo11_deterministic(e,sigma,local_env={},defn_d={},do_log=False,logger_string='',vertex=None):\n",
      "22     \"\"\"\n",
      "23     do not handle sample or observe.\n",
      "24     done in higher level parser of linker function.\n",
      "25     just eval distribution object that gets sampled or observed\n",
      "26     \"\"\"\n",
      "27     # remember to return evaluate (recursive)\n",
      "28         # everytime we call evaluate, we have to use local_env, otherwise it gets overwritten with the default {}\n",
      "29     if do_log: logger.info('ls {}'.format(logger_string))\n",
      "30     if do_log: logger.info('e {}, local_env {}, sigma {}'.format(e, local_env, sigma))\n",
      "31 \n",
      "32     # get first expression out of list or list of one\n",
      "33     if not isinstance(e,list) or len(e) == 1:\n",
      "34         if isinstance(e,list):\n",
      "35             e = e[0]\n",
      "36         if isinstance(e,bool):\n",
      "37             if do_log: logger.info('match case number: e {}, sigma {}'.format(e, sigma))\n",
      "38             return torch.tensor(e), sigma\n",
      "39         if isinstance(e, number):\n",
      "40             if do_log: logger.info('match case number: e {}, sigma {}'.format(e, sigma))\n",
      "41             return torch.tensor(float(e)), sigma\n",
      "42         elif isinstance(e,list):\n",
      "43             if do_log: logger.info('match case list: e {}, sigma {}'.format(e, sigma))\n",
      "44             return e, sigma\n",
      "45         elif e in list(primitives_d.keys()):\n",
      "46             if do_log: logger.info('match case primitives_d: e {}, sigma {}'.format(e, sigma))\n",
      "47             return e, sigma\n",
      "48         elif e in list(distributions_d.keys()):\n",
      "49             if do_log: logger.info('match case distributions_d: e {}, sigma {}'.format(e, sigma))\n",
      "50             return e, sigma\n",
      "51         elif torch.is_tensor(e):\n",
      "52             if do_log: logger.info('match case is_tensor: e {}, sigma {}'.format(e, sigma))\n",
      "53             return e, sigma\n",
      "54         elif e in local_env.keys():\n",
      "55             if do_log: logger.info('match case local_env: e {}, sigma {}'.format(e, sigma))\n",
      "56             if do_log: logger.info('match case local_env: local_env[e] {}'.format(local_env[e]))\n",
      "57             return local_env[e], sigma \n",
      "58         elif e in list(defn_d.keys()):\n",
      "59             if do_log: logger.info('match case defn_d: e {}, sigma {}'.format(e, sigma))\n",
      "60             return e, sigma\n",
      "61         elif isinstance(e,distribution_types):\n",
      "62             if do_log: logger.info('match case distribution: e {}, sigma {}'.format(e,sigma))\n",
      "63             return e, sigma\n",
      "64         else:\n",
      "65             assert False, 'case not matched {}'.format(e)\n",
      "66     \n",
      "67     elif e[0] == 'sample':\n",
      "68         assert False, 'deterministic evaluator'\n",
      "69 \n",
      "70     elif e[0] == 'observe':\n",
      "71         assert False, 'deterministic evaluator'\n",
      "72 \n",
      "73     elif e[0] == 'let': \n",
      "74         if do_log: logger.info('match case let: e {}, sigma {}'.format(e, sigma))\n",
      "75         # let [v1 e1] e0\n",
      "76         # here \n",
      "77             # e[0] : \"let\"\n",
      "78             # e[1] : [v1, e1]\n",
      "79             # e[2] : e0\n",
      "80         # evaluates e1 to c1 and binds this value to e0\n",
      "81         # this means we update the context with old context plus {v1:c1}\n",
      "82         c1, sigma = eval_algo11_deterministic(e[1][1],sigma,local_env,defn_d,do_log=do_log) # evaluates e1 to c1\n",
      "83         v1 = e[1][0]\n",
      "84         return eval_algo11_deterministic(e[2], sigma, local_env = {**local_env,v1:c1},defn_d=defn_d,do_log=do_log)\n",
      "85     elif e[0] == 'if': # if e0 e1 e2\n",
      "86         if do_log: logger.info('match case if: e {}, sigma {}'.format(e, sigma))\n",
      "87         e1 = e[1]\n",
      "88         e2 = e[2]\n",
      "89         e3 = e[3]\n",
      "90         e1_prime, sigma = eval_algo11_deterministic(e1,sigma,local_env,defn_d,do_log=do_log)\n",
      "91         if e1_prime:\n",
      "92             return eval_algo11_deterministic(e2,sigma,local_env,defn_d,do_log=do_log)\n",
      "93         else:\n",
      "94             return eval_algo11_deterministic(e3,sigma,local_env,defn_d,do_log=do_log) \n",
      "95 \n",
      "96     else:\n",
      "97         cs = []\n",
      "98         for ei in e:\n",
      "99             if do_log: logger.info('cycling through expressions: ei {}, sigma {}'.format(ei,sigma))\n",
      "100             c, sigma = eval_algo11_deterministic(ei,sigma,local_env,defn_d,do_log=do_log)\n",
      "101             cs.append(c)\n",
      "102         if cs[0] in primitives_d:\n",
      "103             if do_log: logger.info('do case primitives_d: cs0 {}'.format(cs[0]))\n",
      "104             if do_log: logger.info('do case primitives_d: cs1 {}'.format(cs[1:]))\n",
      "105             if do_log: logger.info('do case primitives_d: primitives_d[cs[0]] {}'.format(primitives_d[cs[0]]))\n",
      "106             return primitives_d[cs[0]](cs[1:]), sigma\n",
      "107         elif cs[0] in distributions_d:\n",
      "108             if do_log: logger.info('do case distributions_d: cs0 {}'.format(cs[0]))\n",
      "109             return distributions_d[cs[0]](cs[1:]), sigma\n",
      "110         elif cs[0] in defn_d:\n",
      "111             if do_log: logger.info('do case defn: cs0  {}'.format(cs[0]))\n",
      "112             defn_function_li = defn_d[cs[0]]\n",
      "113             defn_function_args, defn_function_body = defn_function_li\n",
      "114             local_env_update = {key:value for key,value in zip(defn_function_args, cs[1:])}\n",
      "115             if do_log: logger.info('do case defn: update to local_env from defn_d {}'.format(local_env_update))\n",
      "116             return eval_algo11_deterministic(defn_function_body,sigma,local_env = {**local_env, **local_env_update},defn_d=defn_d,do_log=do_log)\n",
      "117         else:\n",
      "118             assert False, 'not implemented {}'.format(cs)\n",
      "119 \n",
      "120 \n",
      "121 def evaluate_link_function_algo11(P,verteces_topsorted,sigma,local_env,do_log):\n",
      "122     \"\"\"\n",
      "123     evaluates all linking functions in P, using ancestral sampling\n",
      "124     \"\"\"\n",
      "125     for vertex in verteces_topsorted:\n",
      "126         link_function = P[vertex]\n",
      "127         if link_function[0] == 'sample*':\n",
      "128             if do_log: logger_graph.info('match case sample*: link_function {}'.format(link_function))\n",
      "129             assert len(link_function) == 2\n",
      "130             e = link_function[1]\n",
      "131             # because e evaluates to distribution in linking function\n",
      "132             # no sample or observe in eval_algo11\n",
      "133             distribution, sigma = eval_algo11_deterministic(e,sigma,local_env = local_env, do_log=do_log) \n",
      "134             \n",
      "135             # bbvi evaluator algo 11\n",
      "136             # get proposal from sigma\n",
      "137             q = sigma['Q'][vertex]\n",
      "138             constant = q.sample()\n",
      "139             G_v = grad_log_prob(q,constant)\n",
      "140             sigma['G'][vertex] = G_v\n",
      "141             log_wv = score(distribution,constant) - score(q,constant)\n",
      "142             sigma['logW'] += log_wv\n",
      "143             if do_log: logger.info('match case sample: q {}, constant {}, G_v {}, log_wv {}, logW {}'.format(q, constant, G_v,log_wv, sigma['logW']))\n",
      "144             update_local_env = {vertex:constant}\n",
      "145             local_env.update(update_local_env)\n",
      "146 \n",
      "147         elif link_function[0] == 'observe*':\n",
      "148             if do_log: logger_graph.info('match case observe*: link_function {} sigma {}'.format(link_function, sigma))\n",
      "149             e1, e2 = link_function[1:]\n",
      "150             d1, sigma = eval_algo11_deterministic(e1,sigma,local_env,do_log=do_log)\n",
      "151             c2, sigma = eval_algo11_deterministic(e2,sigma,local_env,do_log=do_log)\n",
      "152             log_w = score(d1,c2)\n",
      "153             sigma['logW'] += log_w\n",
      "154             if do_log: logger_graph.info('match case observe*: d1 {}, c2 {}, log_w {}, sigma {}'.format(d1, c2, log_w, sigma))\n",
      "155     \n",
      "156         else:\n",
      "157             assert False\n",
      "158 \n",
      "159     return local_env, sigma\n",
      "160 \n",
      "161 \n",
      "162 def grad_log_prob(distribution_unconst_optim,c):\n",
      "163     \"\"\"TODO: derive these analytically for normal and verify same results\n",
      "164     \"\"\"\n",
      "165     log_prob = distribution_unconst_optim.log_prob(c)\n",
      "166     log_prob.backward()\n",
      "167     lambda_v = distribution_unconst_optim.Parameters()\n",
      "168     D_v = len(lambda_v)\n",
      "169     G_v = torch.zeros(D_v)\n",
      "170     # if D_v==1:\n",
      "171     #     lambda_vd = lambda_v[0]\n",
      "172     #     if \n",
      "173     #     else:\n",
      "174     #         assert False, 'not implemented for lambda_v {}'.format(lambda_v)\n",
      "175 \n",
      "176     for d in range(D_v):\n",
      "177         lambda_v_d = lambda_v[d]\n",
      "178         if lambda_v_d.ndim > 0: # e.g. Categorical concentration\n",
      "179             assert D_v == 1, 'only implemented for multi dimentional lambda_vd for one component, incase different sizes  {}'.format(lambda_v)\n",
      "180             # TODO: generlaize with dict to lambda_vd component, with different sizes\n",
      "181             G_v_d = torch.zeros_like(lambda_v_d)\n",
      "182             G_v_d = lambda_v_d.grad.clone().detach()\n",
      "183             G_v = G_v_d.reshape(1,-1)\n",
      "184             lambda_v_d.grad = None\n",
      "185 \n",
      "186         else:\n",
      "187         \n",
      "188             G_v[d] = lambda_v_d.grad.clone().detach() # seem not to need to do clone().detach(), but keep just in case\n",
      "189             lambda_v_d.grad = None\n",
      "190             # these grads need to be added to lambda_v to make log_prob maximal \n",
      "191             # (backwards because log_prob.backward() assumes log_prob is a loss to be mimimized)\n",
      "192             # we will flip later when we convert these raw per sample gradients to g_hat (over L mini match with b chosen to lower variance)\n",
      "193     return G_v\n",
      "194 \n",
      "195 \n",
      "196 def elbo_gradients(G,logW,union_G_keys):\n",
      "197     \"\"\"\n",
      "198     conversion of per sample gradients in mini match of size L to average gradient g_hat\n",
      "199     b chosen to minimize variance of g_hat\n",
      "200     \"\"\"\n",
      "201     g_hat = {}\n",
      "202     for v in union_G_keys:\n",
      "203         #F_v = []\n",
      "204         G_v = []\n",
      "205         \n",
      "206         L = len(logW)\n",
      "207         for l in range(L):\n",
      "208             G_l = G[l]\n",
      "209             if v in G_l.keys():\n",
      "210                 G_l_v = G_l[v].tolist()\n",
      "211                 G_v.append(G_l_v)\n",
      "212                 D_v = len(G_l_v)\n",
      "213             else:\n",
      "214                 assert False, 'zero not implemented'\n",
      "215         G_v = np.array(G_v)\n",
      "216         # assert G_v.ndim == 2, 'G_v {}, type {}'.format(G_v, type(G_v))\n",
      "217 \n",
      "218 \n",
      "219         # cov and var to compute b_v\n",
      "220         if G_v.ndim == 2:\n",
      "221             F_v = G_v*logW.reshape(-1,1)\n",
      "222             D_v = G_v.shape[1]\n",
      "223             b_v = np.zeros(D_v)\n",
      "224             for d in range(D_v):\n",
      "225                 F_v_d = F_v[:,d]\n",
      "226                 G_v_d = G_v[:,d]\n",
      "227                 cov_F_G = np.cov(F_v_d,G_v_d)\n",
      "228                 b_v[d] = cov_F_G[0,1]/cov_F_G[1,1]\n",
      "229             g_hat_v = (F_v - G_v*b_v).mean(0)  # sum over samples divided by L\n",
      "230             g_hat[v] = g_hat_v\n",
      "231 \n",
      "232         elif G_v.ndim == 3:\n",
      "233             F_v = G_v*logW.reshape(-1,1,1)\n",
      "234             L, D_v, n_D_v = G_v.shape\n",
      "235             assert D_v == 1\n",
      "236             d=0\n",
      "237             b_v_d1 = np.zeros(D_v)\n",
      "238             cov_sum_d1, var_sum_d1 = 0, 0\n",
      "239             for j in range(n_D_v):\n",
      "240                 G_v_1_j = G_v[:,d,j]\n",
      "241                 F_v_1_j = F_v[:,d,j]\n",
      "242                 cov_F_G_j = np.cov(F_v[:,d,j],G_v[:,d,j])\n",
      "243                 cov_sum_d1 += cov_F_G_j[0,1]\n",
      "244                 var_sum_d1 += cov_F_G_j[1,1]\n",
      "245             b_v_d1 = cov_sum_d1 / var_sum_d1\n",
      "246             if np.isnan(b_v_d1):\n",
      "247                 b_v_d1 = 0 # or put a small number in the demonenator\n",
      "248             g_hat_v_d1 = (F_v - G_v*b_v_d1).mean(0)\n",
      "249             g_hat[v] = g_hat_v_d1\n",
      "250             # print('G_v {}, cov_sum_d1 {}, var_sum_d1 {}, g_hat_v_d1 {}'.format(G_v,cov_sum_d1,var_sum_d1,g_hat_v_d1))\n",
      "251 \n",
      "252         else:\n",
      "253             assert False, 'G_v {}, type {}'.format(G_v, type(G_v))\n",
      "254 \n",
      "255     return g_hat\n",
      "256 \n",
      "257 \n",
      "258 def optimizer_step(Q,global_optimizers,g_hat,**kwargs):\n",
      "259     \"\"\"\n",
      "260     no return of Q since modifies in place, and can't deep copy Q, and copy Q still accumulates\n",
      "261     \"\"\"\n",
      "262     for vertex in g_hat.keys():\n",
      "263 \n",
      "264         lambda_v = Q[vertex].Parameters()\n",
      "265         optimizer = global_optimizers[vertex]\n",
      "266         D_v = len(lambda_v)\n",
      "267 \n",
      "268         # TODO: although params already has grad from grad_log_prob, this is not the b adjusted g_hat\n",
      "269         for idx in range(D_v):\n",
      "270             param = lambda_v[idx]\n",
      "271             # param.requires_grad = True # TODO: include???\n",
      "272             param.grad = tensor(-g_hat[vertex][idx],dtype=torch.float32) # TODO: check sign. maximizing\n",
      "273             # Optimizers subtract the gradient of all passed parameters using their .grad attribute as seen here 182. \n",
      "274             # Thus you would minimize the loss using gradient descent.\n",
      "275             # https://discuss.pytorch.org/t/do-optimizers-minimize-or-maximize/69062\n",
      "276 \n",
      "277         optimizer.step() # moves lambda_v\n",
      "278         optimizer.zero_grad() # TODO: need this? \n",
      "279     return Q\n",
      "280 \n",
      "281 \n",
      "282 \n",
      "283 def graph_bbvi_algo12(graph,T,L,sigma=None,init_local_env={},do_log=False,custom_proposals=None,**kwargs):\n",
      "284     \"\"\"This function does ancestral sampling starting from the prior.\n",
      "285     And then ancestral sampling from a learned proposal with bbvi\n",
      "286     TODO: fails when L=1. shapes? averaging?\n",
      "287     \"\"\"\n",
      "288     r, G = [], []\n",
      "289     logW = np.zeros((T,L))\n",
      "290 \n",
      "291     G = graph[1]\n",
      "292     return_of_graph = graph[2] # meaning of program, but need to evaluate\n",
      "293     verteces = G['V']\n",
      "294     arcs = G['A']\n",
      "295     verteces_topsorted = topsort(verteces, arcs)\n",
      "296     P = G['P']\n",
      "297     Y = G['Y']\n",
      "298 \n",
      "299     if (sigma is not None) and ('Q' in sigma) and ('global_optimizers' in sigma):\n",
      "300         pass\n",
      "301 \n",
      "302     else:\n",
      "303     \n",
      "304         # local_env={}\n",
      "305         # if custom_proposals is not None:\n",
      "306         #     for vertex in custom_proposals.keys():\n",
      "307         #         local_env[vertex] = custom_proposals[vertex]\n",
      "308 \n",
      "309         E, sampled_graph = sample_from_joint(graph,local_env=init_local_env,do_log=do_log)\n",
      "310         # now returns Berens' distributions primitives in sampled_graph['prior_dist']\n",
      "311         # print('sampled_graph',sampled_graph)\n",
      "312             \n",
      "313         # initialize once\n",
      "314         sigma={'logW':tensor(0.),'Q':{},'G':{},'global_optimizers':{}}\n",
      "315         for vertex in sampled_graph['prior_dist'].keys():\n",
      "316             if custom_proposals is not None and vertex in custom_proposals.keys():\n",
      "317                 d_prior = custom_proposals[vertex]\n",
      "318             elif vertex not in sigma['Q']:\n",
      "319                 d_prior = sampled_graph['prior_dist'][vertex]\n",
      "320 \n",
      "321             d_prior_withgrads = d_prior.make_copy_with_grads() \n",
      "322                 # only do this once!\n",
      "323                 # no check cases or prior init needed within evaluate_link_function_algo11 etc.\n",
      "324             \n",
      "325             if do_log: logger_graph.info('sigma {}'.format(sigma))\n",
      "326             if do_log: logger_graph.info('custom_proposals {}'.format(custom_proposals))\n",
      "327 \n",
      "328             sigma['Q'][vertex] = d_prior_withgrads\n",
      "329 \n",
      "330 \n",
      "331             # global optimizer\n",
      "332             Q = sigma['Q']\n",
      "333             lambda_v = Q[vertex].Parameters()\n",
      "334             optimizer = torch.optim.Adam(lambda_v, **kwargs)\n",
      "335             sigma['global_optimizers'][vertex] = optimizer\n",
      "336             \n",
      "337 \n",
      "338     if do_log: logger_graph.info('sigma {}'.format(sigma))\n",
      "339 \n",
      "340 \n",
      "341     logW_best = -np.inf\n",
      "342     elbo_best = -np.inf\n",
      "343     sigma['Q_best_t'] = {}\n",
      "344     rand_str = str(np.random.randint(low=0,high=10000000))\n",
      "345     for t in range(T):\n",
      "346         G = []\n",
      "347         r_t=[]\n",
      "348         union_G_keys = set()\n",
      "349 \n",
      "350         for l in range(L):\n",
      "351             sigma={\n",
      "352                 'logW':tensor(0.),\n",
      "353                 'Q':sigma['Q'],\n",
      "354                 'G':{},\n",
      "355                 'global_optimizers':sigma['global_optimizers'],\n",
      "356                 'Q_best_t' : sigma['Q_best_t']\n",
      "357             } # re init gradients\n",
      "358 \n",
      "359             # graph eval algo 11\n",
      "360             local_env, sigma = evaluate_link_function_algo11(P,verteces_topsorted,sigma,local_env={},do_log=do_log)\n",
      "361             sampled_graph = local_env\n",
      "362             r_t_l, sigma = eval_algo11_deterministic(return_of_graph,sigma,local_env=sampled_graph,do_log=do_log)\n",
      "363 \n",
      "364             logW[t,l] = sigma['logW'].item()\n",
      "365             # if logW[t,l] > logW_best:\n",
      "366             #     logW_best = logW[t,l]\n",
      "367             #     sigma['Q_best_t_l'] = sigma['Q']\n",
      "368             G_l = (sigma['G']).copy() # warning for underlying gradients to still tie back to the same objects\n",
      "369             union_G_keys.update(set(G_l.keys()))\n",
      "370             G.append(G_l)\n",
      "371             r_t.append(r_t_l)\n",
      "372             if do_log: logger_graph.info('t {}, l {}, sigma {}, local_env {}'.format(t, l,sigma,local_env))\n",
      "373 \n",
      "374         if do_log: logger_graph.info('sigma {}'.format(sigma))\n",
      "375         g_hat = elbo_gradients(G,logW[t],union_G_keys) \n",
      "376         if do_log: logger_graph.info('g_hat {}, union_G_keys {}'.format(g_hat,union_G_keys))\n",
      "377         Q = sigma['Q']\n",
      "378         if do_log: logger_graph.info('Q before step',Q)\n",
      "379         global_optimizers = sigma['global_optimizers']\n",
      "380         Q = optimizer_step(Q,global_optimizers,g_hat,**kwargs) # in place modification of Q, so Q same as sigma['Q']\n",
      "381         if T <= 10 or t % (T // 10) == 0:\n",
      "382             print('t={}, Q after step={}'.format(t,Q))\n",
      "383 \n",
      "384         r.append(r_t)\n",
      "385 \n",
      "386         elbo = logW[t].mean()\n",
      "387         if elbo > elbo_best:\n",
      "388             elbo_best = elbo\n",
      "389             sigma['Q_best_t'] = torch.save(sigma['Q'],rand_str+'tmp') # cant use copy or deep copy, so just save and load\n",
      "390 \n",
      "391     sigma['Q_best_t'] = torch.load(rand_str+'tmp')\n",
      "392 \n",
      "393     return r, logW, sigma\n",
      "394 \n"
     ]
    }
   ],
   "source": [
    "import bbvi \n",
    "\n",
    "for line_number, function_line in enumerate(getsourcelines(bbvi)[0]):\n",
    "    print(line_number, function_line,end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c9cffc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 def sample_from_joint(graph,sigma=tensor(0.),local_env={'prior_dist':{}},do_log=False,verteces_topsorted=None):\n",
      "1     \"\"\"This function does ancestral sampling starting from the prior.\n",
      "2 \n",
      "3     graph output from `daphne graph -i sugared.daphne`\n",
      "4     * list of length 3\n",
      "5       * first entry is defn dict\n",
      "6         * {\"string-defn-function-name\":[\"fn\", [\"var_1\", ..., \"var_n\"], e_function_body], ...}\n",
      "7       * second entry is graph: {V,A,P,Y}\n",
      "8         * \"V\",\"A\",\"P\",\"Y\" are keys in dict\n",
      "9         * \"V\" : [\"string_name_vertex_1\", ..., \"string_name_vertex_n\"] # list of string names of vertices\n",
      "10         * \"A\" : {\"string_name_vertex_1\" : [..., \"string_name_vertex_i\", ...] # dict of arc pairs (u,v) with u a string key in the dict, and the value a list of string names of the vertices. note that the keys can be things like \"uniform\" and don't have to be vetex name strings\n",
      "11         * \"P\" : \"string_name_vertex_i\" : [\"sample*\", e_i] # dict. keys vertex name strings and value a rested list with a linking function in it. typically e_i is a distribution object. \n",
      "12         * \"Y\" : observes\n",
      "13       * third entry is return\n",
      "14         * name of return rv, or constant\n",
      "15 \n",
      "16     \"\"\"\n",
      "17     G = graph[1]\n",
      "18     verteces = G['V']\n",
      "19     arcs = G['A']\n",
      "20     if verteces_topsorted is None:\n",
      "21         verteces_topsorted = topsort(verteces, arcs)\n",
      "22     else:\n",
      "23         assert set(verteces) == set(verteces_topsorted)\n",
      "24     P = G['P']\n",
      "25     Y = G['Y']\n",
      "26     sampled_graph = {}\n",
      "27     local_env, sigma = evaluate_link_function(P,verteces_topsorted,sigma,local_env=local_env,do_log=do_log)\n",
      "28 \n",
      "29     sampled_graph = local_env\n",
      "30     return_of_graph = graph[2] # meaning of program, but need to evaluate\n",
      "31     # if do_log: print('sample_from_joint local_env',local_env)\n",
      "32     # if do_log: print('sample_from_joint sampled_graph',sampled_graph)\n",
      "33     E = evaluate(return_of_graph,sigma, local_env = sampled_graph, do_log=do_log)\n",
      "34     return E, sampled_graph\n",
      "\n",
      "0 def evaluate_link_function(P,verteces_topsorted,sigma,local_env,do_log):\n",
      "1     if 'prior_dist' not in local_env.keys(): local_env['prior_dist']={}\n",
      "2     for vertex in verteces_topsorted:\n",
      "3         link_function = P[vertex]\n",
      "4         if link_function[0] == 'sample*':\n",
      "5             if do_log: logger_graph.info('match case sample*: link_function {}'.format(link_function))\n",
      "6             assert len(link_function) == 2\n",
      "7             e = link_function[1]\n",
      "8 \n",
      "9             # overwrite\n",
      "10             if 'prior_dist' in local_env.keys() and vertex in local_env['prior_dist'].keys():\n",
      "11                 distribution = local_env['prior_dist'][vertex]\n",
      "12             else:\n",
      "13                 distribution, sigma = evaluate(e,sigma,local_env = local_env, do_log=do_log)\n",
      "14             if do_log: logger_graph.info('match case sample*: distribution {}, sigma {}'.format(sigma, distribution))\n",
      "15             E = distribution.sample() # now have concrete value. need to pass it as var to evaluate\n",
      "16 \n",
      "17             update_local_env = {vertex:E}#{vertex:E, vertex+'_dist':distribution}\n",
      "18             \n",
      "19 \n",
      "20             local_env.update(update_local_env)\n",
      "21             local_env['prior_dist'][vertex] = distribution\n",
      "22         elif link_function[0] == 'observe*':\n",
      "23             if do_log: logger_graph.info('match case observe*: link_function {} sigma {}'.format(link_function, sigma))\n",
      "24             e1, e2 = link_function[1:]\n",
      "25             d1, sigma = evaluate(e1,sigma,local_env,do_log=do_log)\n",
      "26             c2, sigma = evaluate(e2,sigma,local_env,do_log=do_log)\n",
      "27             log_w = score(d1,c2)\n",
      "28             sigma  += log_w\n",
      "29             if do_log: logger_graph.info('match case observe*: d1 {}, c2 {}, log_w {}, sigma {}'.format(d1, c2, log_w, sigma))\n",
      "30         else:\n",
      "31             assert False\n",
      "32 \n",
      "33     return local_env, sigma\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from graph_based_sampling import sample_from_joint, evaluate_link_function\n",
    "\n",
    "list_of_programs = [sample_from_joint, evaluate_link_function]\n",
    "\n",
    "for program in list_of_programs:\n",
    "    for line_number, function_line in enumerate(getsourcelines(program)[0]):\n",
    "        print(line_number, function_line,end='')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fac4d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 class UniformContinuous(dist.Gamma):\n",
      "1     \"\"\"\n",
      "2     Gamma to approx a posterior distribution with support on the positive real line (not including zero)\n",
      "3     \"\"\"\n",
      "4     def __init__(self, low, high, copy=False):\n",
      "5         super().__init__(concentration=low,\n",
      "6                              rate=high)\n",
      "7 \n",
      "8     def Parameters(self):\n",
      "9         \"\"\"Return a list of parameters for the distribution\"\"\"\n",
      "10         return [self.concentration, self.rate]\n",
      "11 \n",
      "12     def make_copy_with_grads(self):\n",
      "13         \"\"\"\n",
      "14         Return a copy  of the distribution, with parameters that require_grad\n",
      "15         \"\"\"\n",
      "16 \n",
      "17         ps = [p.clone().detach().requires_grad_() for p in self.Parameters()]\n",
      "18 \n",
      "19         return UniformContinuous(*ps, copy=True)\n",
      "20 \n",
      "21     def log_prob(self, x):\n",
      "22 \n",
      "23         return super().log_prob(x)\n"
     ]
    }
   ],
   "source": [
    "from distributions import UniformContinuous\n",
    "\n",
    "for line_number, function_line in enumerate(getsourcelines(UniformContinuous)[0]):\n",
    "    print(line_number, function_line,end='')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
